{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "F1 with Rekall labels (1 seed, 25 epochs, 16 batch size):\n",
    "* 0.8376 @ 5%\n",
    "* 0.8989 @ 10%\n",
    "* 0.9107 @ 20%\n",
    "* 0.9240 @ 40%\n",
    "* 0.9233 @ 60%\n",
    "* 0.9243 @ 80%\n",
    "* 0.9279 @ 100%\n",
    "* 0.9368 @ 200% {'train': 141661, 'val_train': 9479, 'val': 9479, 'test': 7496}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup (for Windows 10)**\n",
    "\n",
    "1. Create a Python 3.7 'pytorch' environment with Anaconda Navigator\n",
    "2. Activate the environment and run 'install pytorch torchvision - c pytorch': https://medium.com/@iamHarin17/how-to-setup-a-python-environment-for-deep-learning-with-anaconda-f65ab78a362\n",
    "3. Run 'pip install' for 'matplotlib', 'tqdm' and 'scipy'\n",
    "4. Install Jupyter Notebook from Anaconda Navigator and open this notebook.\n",
    "\n",
    "**Helpful Links**\n",
    "\n",
    "DOS Nvidia GPU usage: https://wiki.tuflow.com/index.php?title=DOS_GPU_Usage\n",
    "\n",
    "cd C:\\Program Files\\NVIDIA Corporation\\NVSMI\n",
    "\n",
    "nvidia-smi -l 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set global variables to run notebook\n",
    "\n",
    "# data root\n",
    "data_root = 'C:\\\\_data\\\\' # set to whever the data is being stored\n",
    "\n",
    "# hyperparameters\n",
    "# with m2200 - 15min with 8, 10min with 16, 9min with 24 and out of memory with 32\n",
    "global_batch_size = 16 # originally set to 4, but we've agreed to use 16 as default\n",
    "\n",
    "# parameters to control length of training\n",
    "use_rekall_labels = True # False will use 'true' labels - files need to be in 'commercials\\\\data\\\\'\n",
    "use_big_dataset   = True # True uses 'rekall_labels_big' and False uses 'rekall_labels'\n",
    "data_percentage = 2    # percentage of training data to use\n",
    "num_seeds = 1            # number of different seeds to run, originally 5\n",
    "global_num_epochs = 25   # originally 25\n",
    "\n",
    "# BrokenPipeError on Windows 10\n",
    "# https://discuss.pytorch.org/t/brokenpipeerror-errno-32-broken-pipe-when-i-run-cifar10-tutorial-py/6224/3\n",
    "global_num_workers = 0 # originally set to 4, so perhaps should be 4 on Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "# #         transforms.Resize((310, 310)),\n",
    "#         transforms.Resize((224, 224)),\n",
    "# #         transforms.CenterCrop(224),\n",
    "# #         transforms.RandomResizedCrop(224),\n",
    "# #         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "# #         transforms.Resize((310, 310)),\n",
    "#         transforms.Resize((224, 224)),\n",
    "# #         transforms.Resize(256),\n",
    "# #         transforms.CenterCrop(224),\n",
    "# #         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, paths, labels, transform=None, stride = 1, max_size=None):\n",
    "        self.paths = paths[::stride]\n",
    "        if max_size is not None:\n",
    "            self.paths = self.paths[:max_size]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.paths[idx]\n",
    "        img = Image.open(img_name).convert('RGB')        \n",
    "        img_tensor = self.transform(img)\n",
    "        return img_tensor, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with  C:\\_data\\commercials\\data\\rekall_train.txt\n"
     ]
    }
   ],
   "source": [
    "# Load files used to identify train, validation and test data\n",
    "\n",
    "if use_rekall_labels: # use Rekall label for training\n",
    "    train_file = os.path.join(data_root, 'commercials\\\\data\\\\rekall_train.txt')\n",
    "    if use_big_dataset:\n",
    "        train_file_new_points = os.path.join(data_root, 'commercials\\\\data\\\\rekall_big_train_new.txt')\n",
    "else:\n",
    "    train_file = os.path.join(data_root, 'commercials\\\\data\\\\train.txt')\n",
    "\n",
    "# continue to use true labels for validation and test\n",
    "val_file   = os.path.join(data_root, 'commercials\\\\data\\\\val.txt')\n",
    "test_file  = os.path.join(data_root, 'commercials\\\\data\\\\test.txt')\n",
    "\n",
    "print('Training with ', train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, filename_new_points = None, load_percentage = 1):\n",
    "    # Assumes that there is no overlap in datapoints between filename and filename_new_points!\n",
    "    paths = []\n",
    "    labels = []\n",
    "    videos = []\n",
    "    \n",
    "    num_total_videos = 0\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        \n",
    "        # find set of unique videos on file\n",
    "        for line in f.readlines():\n",
    "            video, image, label = line.split(' ')\n",
    "            videos.append(video)\n",
    "        videos = sorted(list(set(videos))) # set of all videos\n",
    "        num_total_videos = len(videos)\n",
    "        num_videos = int(len(videos) * load_percentage) # number of videos to use\n",
    "        videos = videos[:num_videos] # take first 'num_videos'\n",
    "        f.seek(0) # go back to beginning of file\n",
    "        \n",
    "        # read in data only using videos in possibly reduced set from above\n",
    "        for line in f.readlines():\n",
    "            video, image, label = line.split(' ')\n",
    "            if video in videos: # load it up only if it's in the list of videos to use\n",
    "                paths.append(data_root + 'commercials/images/{}/{:04d}.jpg'.format(video, int(image)))\n",
    "                labels.append(int(label))\n",
    "             \n",
    "    videos = []\n",
    "    if filename_new_points is not None and load_percentage > 1:\n",
    "        print(\"Fetching points from new data file\")\n",
    "        with open(filename_new_points, 'r') as f:\n",
    "\n",
    "            # find set of unique videos on file\n",
    "            for line in f.readlines():\n",
    "                video, image, label = line.split(' ')\n",
    "                videos.append(video)\n",
    "            videos = sorted(list(set(videos))) # set of all videos\n",
    "            num_videos_to_fetch = int(num_total_videos * (load_percentage - 1))\n",
    "            if num_videos_to_fetch > len(videos):\n",
    "                print(\"Trying to fetch a larger data_percentage than is available\")\n",
    "                num_videos_to_fetch = len(videos)\n",
    "            videos = videos[:num_videos_to_fetch] # take first 'num_videos'\n",
    "            f.seek(0) # go back to beginning of file\n",
    "\n",
    "            # read in data only using videos in possibly reduced set from above\n",
    "            for line in f.readlines():\n",
    "                video, image, label = line.split(' ')\n",
    "                if video in videos: # load it up only if it's in the list of videos to use\n",
    "                    paths.append(data_root + 'commercials/images/{}/{:04d}.jpg'.format(video, int(image)))\n",
    "                    labels.append(int(label))\n",
    "    \n",
    "    print('total videos being loaded: ', num_total_videos)\n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching points from new data file\n",
      "total videos being loaded:  168\n",
      "total videos being loaded:  21\n",
      "total videos being loaded:  21\n"
     ]
    }
   ],
   "source": [
    "# Load up the data for train, validation and test\n",
    "train_paths, Y_train = read_file(train_file, train_file_new_points, load_percentage=data_percentage)\n",
    "val_paths  , Y_val   = read_file(val_file)\n",
    "test_paths , Y_test  = read_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_train is the same as val\n",
    "image_datasets = {\n",
    "    'train'    : ImageDataset(train_paths, Y_train, transform = data_transforms['train'], stride=1, max_size=None),\n",
    "    'val_train': ImageDataset(val_paths  , Y_val  , transform = data_transforms['train'], stride=1, max_size=None),\n",
    "    'val'      : ImageDataset(val_paths  , Y_val  , transform = data_transforms['val']  , stride=1, max_size=None),\n",
    "    'test'     : ImageDataset(test_paths , Y_test , transform = data_transforms['val']  , stride=1, max_size=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                              batch_size = global_batch_size,\n",
    "                                              shuffle = 'train' in x,\n",
    "                                              num_workers = global_num_workers,\n",
    "                                              pin_memory = True)\n",
    "                                              for x in image_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 141661, 'val_train': 9479, 'val': 9479, 'test': 7496}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets\n",
    "}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    return a / b if b > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_data(model, criterion, optimizer, scheduler, dl, phase, epoch, num_epochs, verbose=True, log_file=None):\n",
    "    \n",
    "    running_loss = 0.\n",
    "    running_corrects = 0\n",
    "    true_positives = 0.\n",
    "    true_negatives = 0.\n",
    "    false_positives = 0.\n",
    "    false_negatives = 0.\n",
    "    \n",
    "    # load saved model if we're not starting from the beginning\n",
    "    if epoch != 0:\n",
    "        model.load_state_dict(torch.load(log_file.split('.')[0] + '.pth'))\n",
    "\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dl:\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.where(\n",
    "                outputs >= 0.,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "            target = torch.where(\n",
    "                labels >= 0.5,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device)\n",
    "            )\n",
    "            loss = criterion(outputs.view(target.shape), target)\n",
    "\n",
    "            # backward + optimize only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        label_vals = torch.where(\n",
    "            labels >= 0.5,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device)\n",
    "        )\n",
    "        correct = preds.view(label_vals.shape) == label_vals.data\n",
    "        running_corrects += torch.sum(correct)\n",
    "\n",
    "        true_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        true_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "\n",
    "    dataset_size = len(inputs)\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_acc = running_corrects.double() / dataset_size\n",
    "    epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "    epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "    epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "    if verbose:\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "            phase, epoch_loss, epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "        print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "            true_positives.data, true_negatives.data, false_positives.data, false_negatives.data))\n",
    "    \n",
    "    # Write statistics to log file and save model\n",
    "    if log_file is not None:\n",
    "        with open(log_file, 'a+') as f:\n",
    "            f.write('Phase: {0}\\t'\n",
    "                       'Epoch: [{1}/{2}]\\t'\n",
    "                       'Loss: {loss_c:.4f}\\t'\n",
    "                       'Acc: {acc:.4f}\\t'\n",
    "                       'Pre: {pre:.4f}\\t'\n",
    "                       'Rec: {rec:.4f}\\t'\n",
    "                       'F1: {f1:.4f}\\t'\n",
    "                       'TP: {tp} '\n",
    "                       'TN: {tn} '\n",
    "                       'FP: {fp} '\n",
    "                       'FN: {fn}\\n'.format(\n",
    "                           phase, epoch + 1, num_epochs, loss_c=epoch_loss,\n",
    "                           acc=epoch_acc, pre=epoch_pre, rec=epoch_recall,\n",
    "                           f1=epoch_f1, tp=int(true_positives.data), tn=int(true_negatives.data),\n",
    "                           fp=int(false_positives.data), fn=int(false_negatives.data)\n",
    "                       ))\n",
    "            f.flush()\n",
    "        \n",
    "        # Save model at end of epoch\n",
    "        torch.save(model.state_dict(), log_file.split('.')[0] + '.pth')\n",
    "\n",
    "#     # deep copy the model\n",
    "#     if phase == 'val' and epoch_f1 > best_f1:\n",
    "#         best_acc = epoch_acc\n",
    "#         best_f1 = epoch_f1\n",
    "#         best_precision = epoch_pre\n",
    "#         best_recall = epoch_recall\n",
    "#         best_epoch = epoch\n",
    "#         best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     if phase == 'test' and best_epoch == epoch:\n",
    "#         best_test_acc = epoch_acc\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, train_dl, val_dl, test_dl=None,\n",
    "                num_epochs=25, return_best=False, verbose=True, log_file=None):\n",
    "    \n",
    "    print(train_dl, val_dl, test_dl)\n",
    "    since = time.time() # timestamp for beginning of training\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    best_test_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    phases = ['train', 'val', 'test'] if test_dl is not None else ['train', 'val']\n",
    "    \n",
    "    # retrieve last trained epoch\n",
    "    last_epoch = 0\n",
    "    if (log_file is not None) and os.path.exists(log_file): # make sure the file exists first\n",
    "        with open(log_file, 'r') as f:\n",
    "            for line in f: # interate through file to get last line\n",
    "                last = line\n",
    "        last_epoch = int(last.split('[')[1].split('/')[0]) # find digits between first '[' and '/'\n",
    "\n",
    "    for epoch in range(last_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        if epoch != num_epochs - 1:\n",
    "            \n",
    "            # only train if it's not the last epoch\n",
    "            scheduler.step()\n",
    "            model.train()  # Set model to training mode\n",
    "            dl = dataloaders[train_dl]\n",
    "            dataset_size = dataset_sizes[train_dl]\n",
    "            pass_data(model, criterion, optimizer, scheduler, dl, 'train', epoch, num_epochs, verbose, log_file)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # on final epoch run train, validation and test\n",
    "            for phase in phases:\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "                    model.train()  # Set model to training mode\n",
    "                    dl = dataloaders[train_dl]\n",
    "                    dataset_size = dataset_sizes[train_dl]                \n",
    "                elif phase == 'val':\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dl = dataloaders[val_dl]\n",
    "                    dataset_size = dataset_sizes[val_dl]\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "                    dl = dataloaders[test_dl]\n",
    "                    dataset_size = dataset_sizes[test_dl]\n",
    "                pass_data(model, criterion, optimizer, scheduler, dl, phase, epoch, num_epochs, verbose, log_file)\n",
    "\n",
    "        # print end-of-epoch stats\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training time {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print()\n",
    "\n",
    "    # print total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print('\\nTraining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    if return_best:\n",
    "        print('Best epoch: {}'.format(best_epoch))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "        print('Best val Pre: {:4f}'.format(best_precision))\n",
    "        print('Best val Rec: {:4f}'.format(best_recall))\n",
    "        print('Best val F1: {:4f}'.format(best_f1))\n",
    "        print('Test Acc: {:4f}'.format(best_test_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train val test\n",
      "Epoch 0/24\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcram\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 2952.6709 Acc: 9816.3077 Pre: 0.8311 Rec: 0.8221 F1: 0.8266\n",
      "TP: 33484.0 TN: 94128.0 FP: 6803.0 FN: 7246.0\n",
      "Training time 106m 54s\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2598.6002 Acc: 9982.9231 Pre: 0.8578 Rec: 0.8490 F1: 0.8534\n",
      "TP: 34580.0 TN: 95198.0 FP: 5733.0 FN: 6150.0\n",
      "Training time 212m 59s\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 2483.3066 Acc: 10028.6923 Pre: 0.8635 Rec: 0.8586 F1: 0.8610\n",
      "TP: 34970.0 TN: 95403.0 FP: 5528.0 FN: 5760.0\n",
      "Training time 318m 40s\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 2412.6476 Acc: 10055.8462 Pre: 0.8684 Rec: 0.8622 F1: 0.8653\n",
      "TP: 35119.0 TN: 95607.0 FP: 5324.0 FN: 5611.0\n",
      "Training time 423m 12s\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 2335.2399 Acc: 10077.3846 Pre: 0.8711 Rec: 0.8666 F1: 0.8689\n",
      "TP: 35298.0 TN: 95708.0 FP: 5223.0 FN: 5432.0\n",
      "Training time 527m 43s\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 2284.0445 Acc: 10094.7692 Pre: 0.8752 Rec: 0.8676 F1: 0.8714\n",
      "TP: 35339.0 TN: 95893.0 FP: 5038.0 FN: 5391.0\n",
      "Training time 632m 6s\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 2138.0518 Acc: 10139.0000 Pre: 0.8816 Rec: 0.8757 F1: 0.8786\n",
      "TP: 35667.0 TN: 96140.0 FP: 4791.0 FN: 5063.0\n",
      "Training time 736m 24s\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 2087.3822 Acc: 10161.1538 Pre: 0.8859 Rec: 0.8782 F1: 0.8821\n",
      "TP: 35771.0 TN: 96324.0 FP: 4607.0 FN: 4959.0\n",
      "Training time 840m 53s\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 2059.2082 Acc: 10172.4615 Pre: 0.8876 Rec: 0.8803 F1: 0.8839\n",
      "TP: 35853.0 TN: 96389.0 FP: 4542.0 FN: 4877.0\n",
      "Training time 945m 20s\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 2039.9439 Acc: 10165.8462 Pre: 0.8863 Rec: 0.8795 F1: 0.8829\n",
      "TP: 35821.0 TN: 96335.0 FP: 4596.0 FN: 4909.0\n",
      "Training time 1050m 0s\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 2018.3429 Acc: 10183.4615 Pre: 0.8891 Rec: 0.8822 F1: 0.8857\n",
      "TP: 35934.0 TN: 96451.0 FP: 4480.0 FN: 4796.0\n",
      "Training time 1154m 40s\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1998.9886 Acc: 10184.4615 Pre: 0.8895 Rec: 0.8821 F1: 0.8858\n",
      "TP: 35928.0 TN: 96470.0 FP: 4461.0 FN: 4802.0\n",
      "Training time 1259m 54s\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1979.6058 Acc: 10194.6923 Pre: 0.8914 Rec: 0.8834 F1: 0.8874\n",
      "TP: 35982.0 TN: 96549.0 FP: 4382.0 FN: 4748.0\n",
      "Training time 1371m 25s\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1966.8598 Acc: 10195.8462 Pre: 0.8898 Rec: 0.8860 F1: 0.8879\n",
      "TP: 36085.0 TN: 96461.0 FP: 4470.0 FN: 4645.0\n",
      "Training time 1476m 40s\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1963.4471 Acc: 10204.2308 Pre: 0.8926 Rec: 0.8854 F1: 0.8890\n",
      "TP: 36061.0 TN: 96594.0 FP: 4337.0 FN: 4669.0\n",
      "Training time 1581m 58s\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1948.8982 Acc: 10199.7692 Pre: 0.8916 Rec: 0.8850 F1: 0.8883\n",
      "TP: 36047.0 TN: 96550.0 FP: 4381.0 FN: 4683.0\n",
      "Training time 1687m 18s\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1951.7613 Acc: 10204.1538 Pre: 0.8926 Rec: 0.8854 F1: 0.8890\n",
      "TP: 36064.0 TN: 96590.0 FP: 4341.0 FN: 4666.0\n",
      "Training time 1792m 25s\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1942.9993 Acc: 10192.0000 Pre: 0.8906 Rec: 0.8835 F1: 0.8870\n",
      "TP: 35985.0 TN: 96511.0 FP: 4420.0 FN: 4745.0\n",
      "Training time 1897m 37s\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1933.2706 Acc: 10203.0769 Pre: 0.8930 Rec: 0.8845 F1: 0.8887\n",
      "TP: 36027.0 TN: 96613.0 FP: 4318.0 FN: 4703.0\n",
      "Training time 2002m 35s\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1934.6584 Acc: 10200.4615 Pre: 0.8921 Rec: 0.8847 F1: 0.8884\n",
      "TP: 36034.0 TN: 96572.0 FP: 4359.0 FN: 4696.0\n",
      "Training time 2107m 22s\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1922.9189 Acc: 10210.9231 Pre: 0.8933 Rec: 0.8869 F1: 0.8901\n",
      "TP: 36125.0 TN: 96617.0 FP: 4314.0 FN: 4605.0\n",
      "Training time 2212m 22s\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1938.1792 Acc: 10208.9231 Pre: 0.8931 Rec: 0.8865 F1: 0.8898\n",
      "TP: 36107.0 TN: 96609.0 FP: 4322.0 FN: 4623.0\n",
      "Training time 2317m 8s\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1941.4645 Acc: 10197.9231 Pre: 0.8907 Rec: 0.8855 F1: 0.8881\n",
      "TP: 36068.0 TN: 96505.0 FP: 4426.0 FN: 4662.0\n",
      "Training time 2422m 8s\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1928.7525 Acc: 10208.5385 Pre: 0.8937 Rec: 0.8856 F1: 0.8896\n",
      "TP: 36071.0 TN: 96640.0 FP: 4291.0 FN: 4659.0\n",
      "Training time 2533m 20s\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1942.7724 Acc: 10208.9231 Pre: 0.8934 Rec: 0.8861 F1: 0.8897\n",
      "TP: 36089.0 TN: 96627.0 FP: 4304.0 FN: 4641.0\n",
      "val Loss: 127.4414 Acc: 1314.8571 Pre: 0.9429 Rec: 0.9583 F1: 0.9505\n",
      "TP: 2643.0 TN: 6561.0 FP: 160.0 FN: 115.0\n",
      "test Loss: 121.2410 Acc: 899.5000 Pre: 0.9360 Rec: 0.9376 F1: 0.9368\n",
      "TP: 2223.0 TN: 4973.0 FP: 152.0 FN: 148.0\n",
      "Training time 2643m 45s\n",
      "\n",
      "\n",
      "Training complete in 2643m 45s\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(data_root, 'Commercials\\\\models\\\\transfer_learning')\n",
    "for seed in range(num_seeds):\n",
    "    torch.manual_seed(seed)\n",
    "    model_ts = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ts.fc.in_features\n",
    "    model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "    model_ts = model_ts.to(device)\n",
    "\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.]).to(device))\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.]).to(device))\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ts = optim.SGD(model_ts.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler_ts = lr_scheduler.StepLR(optimizer_ts, step_size=7, gamma=0.1)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    log_file = os.path.join(path, 'seed_{}.log'.format(seed))\n",
    "    model_ts = train_model(model_ts, criterion, optimizer_ts, exp_lr_scheduler_ts,\n",
    "                               'train', 'val', test_dl='test', num_epochs=global_num_epochs, verbose=True,\n",
    "                               log_file=log_file, return_best=False)\n",
    "    torch.save(model_ts.state_dict(), os.path.join(path, 'seed_{}.pth'.format(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_negs(predictions, gt):\n",
    "    correct = np.where(np.array(predictions) == np.array(gt), 1, 0)\n",
    "    incorrect = np.where(np.array(predictions) == np.array(gt), 0, 1)\n",
    "    \n",
    "    tp = np.where(correct * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    tn = np.where(correct * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    fp = np.where(incorrect * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    fn = np.where(incorrect * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def acc_prf1(predictions, gt):\n",
    "    tp, tn, fp, fn = pos_negs(predictions, gt)\n",
    "    \n",
    "    acc = (np.sum(tp) + np.sum(tn)) / (np.sum(tp) + np.sum(tn) + np.sum(fp) + np.sum(fn))\n",
    "    precision = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return acc, precision, recall, f1, np.sum(tp), np.sum(tn), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def smooth_predictions(preds, window_radius = 3):\n",
    "    result = []\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_radius)\n",
    "        end = min(len(preds), i + window_radius)\n",
    "        window = preds[start:end]\n",
    "        result += [max(window, key=window.count)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on seed: 0\n",
      "Acc: 0.9600 Pre: 0.9360 Rec: 0.9376 F1: 0.9368\n",
      "TP: 2223.0 TN: 4973.0 FP: 152.0 FN: 148.0\n",
      "Smoothed stats:\n",
      "(0.9494397011739595, 0.9227504244482173, 0.9169126950653732, 0.9198222974402369, 2174, 4943, 182, 197)\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(data_root, 'Commercials\\\\models\\\\transfer_learning')\n",
    "log_file_img = open(os.path.join(path, 'test_results_image_classifier.log'), 'w')\n",
    "log_file_smoothed = open(os.path.join(path, 'test_results_smoothed.log'), 'w')\n",
    "\n",
    "for seed in range(num_seeds):\n",
    "    print('Working on seed:', seed)\n",
    "    \n",
    "    model_ts = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ts.fc.in_features\n",
    "    model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "    model_ts.load_state_dict(torch.load(os.path.join(path, 'seed_{}.pth'.format(seed))))\n",
    "    model_ts.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model = model_ts.eval()   # Set model to evaluate mode\n",
    "    dl = dataloaders['test']\n",
    "    dataset_size = dataset_sizes['test']\n",
    "    \n",
    "    running_corrects = 0\n",
    "    true_positives = 0.\n",
    "    true_negatives = 0.\n",
    "    false_positives = 0.\n",
    "    false_negatives = 0.\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    predictions = []\n",
    "    gt_labels = []\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dl:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.where(\n",
    "                outputs >= 0.,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "            target = torch.where(\n",
    "                labels >= 0.5,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device)\n",
    "            )\n",
    "\n",
    "        predictions += preds.cpu().numpy().tolist()\n",
    "        gt_labels += labels.cpu().numpy().tolist()\n",
    "\n",
    "        # statistics\n",
    "        label_vals = torch.where(\n",
    "            labels >= 0.5,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device)\n",
    "        )\n",
    "        correct = preds.view(label_vals.shape) == label_vals.data\n",
    "        running_corrects += torch.sum(correct)\n",
    "\n",
    "        true_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        true_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "\n",
    "        num_fp = torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "        )\n",
    "\n",
    "    #     if num_fp > 0:\n",
    "    #         print(num_fp)\n",
    "    #         print(torch.where(\n",
    "    #             (correct == 0.) * (label_vals == 0.),\n",
    "    #             torch.tensor([1.]).to(device),\n",
    "    #             torch.tensor([0.]).to(device)))\n",
    "    #         print(i)\n",
    "    #         out = torchvision.utils.make_grid(inputs)\n",
    "    # #         imshow(out, title=preds.cpu().numpy().tolist())\n",
    "    #         imshow(out)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    epoch_acc = running_corrects.double() / dataset_size\n",
    "    epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "    epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "    epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "    print('Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "        epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "    print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "        true_positives.data, true_negatives.data, false_positives.data, \n",
    "        false_negatives.data))\n",
    "    \n",
    "    log_file_img.write('Seed: {0}\\t'\n",
    "                   'Acc: {acc:.4f}\\t'\n",
    "                   'Pre: {pre:.4f}\\t'\n",
    "                   'Rec: {rec:.4f}\\t'\n",
    "                   'F1: {f1:.4f}\\t'\n",
    "                   'TP: {tp} '\n",
    "                   'TN: {tn} '\n",
    "                   'FP: {fp} '\n",
    "                   'FN: {fn}\\n'.format(\n",
    "                       seed,\n",
    "                       acc=epoch_acc, pre=epoch_pre, rec=epoch_recall,\n",
    "                       f1=epoch_f1, tp=int(true_positives.data),\n",
    "                       tn=int(true_negatives.data),\n",
    "                       fp=int(false_positives.data), fn=int(false_negatives.data)\n",
    "                   ))\n",
    "    log_file_img.flush()\n",
    "\n",
    "    predictions = [p[0] for p in predictions]\n",
    "\n",
    "    smoothed_preds = smooth_predictions(predictions, 3)\n",
    "\n",
    "    print(\"Smoothed stats:\")\n",
    "    print(acc_prf1(smoothed_preds, gt_labels))\n",
    "    \n",
    "    sm_acc, sm_pre, sm_rec, sm_f1, sm_tp, sm_tn, sm_fp, sm_fn = acc_prf1(\n",
    "        smoothed_preds, gt_labels)\n",
    "    \n",
    "    log_file_smoothed.write('Seed: {0}\\t'\n",
    "                   'Acc: {acc:.4f}\\t'\n",
    "                   'Pre: {pre:.4f}\\t'\n",
    "                   'Rec: {rec:.4f}\\t'\n",
    "                   'F1: {f1:.4f}\\t'\n",
    "                   'TP: {tp} '\n",
    "                   'TN: {tn} '\n",
    "                   'FP: {fp} '\n",
    "                   'FN: {fn}\\n'.format(\n",
    "                       seed,\n",
    "                       acc=sm_acc, pre=sm_pre, rec=sm_rec,\n",
    "                       f1=sm_f1, tp=sm_tp,\n",
    "                       tn=sm_tn,\n",
    "                       fp=sm_fp, fn=sm_fn\n",
    "                   ))\n",
    "    log_file_smoothed.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(data_root, 'Commercials\\\\models\\\\transfer_learning')\n",
    "model_ts = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ts.fc.in_features\n",
    "model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "model_ts.load_state_dict(torch.load(os.path.join(path, 'seed_0.pth')))\n",
    "model_ts.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ts.eval()   # Set model to evaluate mode\n",
    "dl = dataloaders['test']\n",
    "dataset_size = dataset_sizes['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(str(title))\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_negs(predictions, gt):\n",
    "    correct = np.where(np.array(predictions) == np.array(gt), 1, 0)\n",
    "    incorrect = np.where(np.array(predictions) == np.array(gt), 0, 1)\n",
    "    \n",
    "    tp = np.where(correct * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    tn = np.where(correct * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    fp = np.where(incorrect * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    fn = np.where(incorrect * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def acc_prf1(predictions, gt):\n",
    "    tp, tn, fp, fn = pos_negs(predictions, gt)\n",
    "    \n",
    "    acc = (np.sum(tp) + np.sum(tn)) / (np.sum(tp) + np.sum(tn) + np.sum(fp) + np.sum(fn))\n",
    "    precision = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return acc, precision, recall, f1, np.sum(tp), np.sum(tn), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def smooth_predictions(preds, window_radius = 3):\n",
    "    result = []\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_radius)\n",
    "        end = min(len(preds), i + window_radius)\n",
    "        window = preds[start:end]\n",
    "        result += [max(window, key=window.count)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9448 Pre: 0.9086 Rec: 0.9178 F1: 0.9131\n",
      "TP: 2176.0 TN: 4906.0 FP: 219.0 FN: 195.0\n",
      "Smoothed stats:\n",
      "(0.941435432230523, 0.90418410041841, 0.9114297764656263, 0.9077924805713086, 2161, 4896, 229, 210)\n"
     ]
    }
   ],
   "source": [
    "running_corrects = 0\n",
    "true_positives = 0.\n",
    "true_negatives = 0.\n",
    "false_positives = 0.\n",
    "false_negatives = 0.\n",
    "\n",
    "i = 0\n",
    "\n",
    "predictions = []\n",
    "gt_labels = []\n",
    "# Iterate over data.\n",
    "for inputs, labels in dl:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device).float()\n",
    "\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.where(\n",
    "            outputs >= 0.,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "        target = torch.where(\n",
    "            labels >= 0.5,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device)\n",
    "        )\n",
    "        \n",
    "    predictions += preds.cpu().numpy().tolist()\n",
    "    gt_labels += labels.cpu().numpy().tolist()\n",
    "\n",
    "    # statistics\n",
    "    label_vals = torch.where(\n",
    "        labels >= 0.5,\n",
    "        torch.tensor([1.]).to(device),\n",
    "        torch.tensor([0.]).to(device)\n",
    "    )\n",
    "    correct = preds.view(label_vals.shape) == label_vals.data\n",
    "    running_corrects += torch.sum(correct)\n",
    "\n",
    "    true_positives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 1.) * (label_vals == 1.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    true_negatives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 1.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    false_positives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    false_negatives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 1.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    \n",
    "    num_fp = torch.sum(\n",
    "    torch.where(\n",
    "        (correct == 0.) * (label_vals == 0.),\n",
    "        torch.tensor([1.]).to(device),\n",
    "        torch.tensor([0.]).to(device))\n",
    "    )\n",
    "\n",
    "#     if num_fp > 0:\n",
    "#         print(num_fp)\n",
    "#         print(torch.where(\n",
    "#             (correct == 0.) * (label_vals == 0.),\n",
    "#             torch.tensor([1.]).to(device),\n",
    "#             torch.tensor([0.]).to(device)))\n",
    "#         print(i)\n",
    "#         out = torchvision.utils.make_grid(inputs)\n",
    "# #         imshow(out, title=preds.cpu().numpy().tolist())\n",
    "#         imshow(out)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "epoch_acc = running_corrects.double() / dataset_size\n",
    "epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "print('Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "    epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "    true_positives.data, true_negatives.data, false_positives.data, false_negatives.data))\n",
    "\n",
    "predictions = [p[0] for p in predictions]\n",
    "\n",
    "smoothed_preds = smooth_predictions(predictions, 3)\n",
    "\n",
    "print(\"Smoothed stats:\")\n",
    "print(acc_prf1(smoothed_preds, gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
