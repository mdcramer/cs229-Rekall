{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "# #         transforms.Resize((310, 310)),\n",
    "#         transforms.Resize((224, 224)),\n",
    "# #         transforms.CenterCrop(224),\n",
    "# #         transforms.RandomResizedCrop(224),\n",
    "# #         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "# #         transforms.Resize((310, 310)),\n",
    "#         transforms.Resize((224, 224)),\n",
    "# #         transforms.Resize(256),\n",
    "# #         transforms.CenterCrop(224),\n",
    "# #         transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None, stride = 1, max_size=None):\n",
    "        self.paths = paths[::stride]\n",
    "        if max_size is not None:\n",
    "            self.paths = self.paths[:max_size]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.paths[idx]\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        return img_tensor, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'commercials/data/train.txt'\n",
    "val_file = 'commercials/data/val.txt'\n",
    "test_file = 'commercials/data/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    paths = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            video, image, label = line.split(' ')\n",
    "            paths.append('commercials/images/{}/{:04d}.jpg'.format(video, int(image)))\n",
    "            labels.append(int(label))\n",
    "            \n",
    "    return paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, Y_train = read_file(train_file)\n",
    "val_paths, Y_val = read_file(val_file)\n",
    "test_paths, Y_test = read_file(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {\n",
    "    'train': ImageDataset(train_paths, Y_train, transform = data_transforms['train'],\n",
    "                          stride=1, max_size=None),\n",
    "    'val_train': ImageDataset(val_paths, Y_val, transform = data_transforms['train'],\n",
    "                       stride=1, max_size=None),\n",
    "    'val': ImageDataset(val_paths, Y_val, transform = data_transforms['val'],\n",
    "                       stride=1, max_size=None),\n",
    "    'test': ImageDataset(test_paths, Y_test, transform = data_transforms['val'],\n",
    "                        stride=1, max_size=None)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x],\n",
    "                                              batch_size=4,\n",
    "                                              shuffle='train' in x,\n",
    "                                              num_workers=4,\n",
    "                                              pin_memory=True)\n",
    "                                              for x in image_datasets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x])\n",
    "    for x in image_datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 64130, 'val_train': 9479, 'val': 9479, 'test': 7496}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    return a / b if b > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train_dl, val_dl, test_dl=None,\n",
    "                num_epochs=25, return_best=False, verbose=True, log_file=None):\n",
    "    print(train_dl, val_dl, test_dl)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    best_test_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    \n",
    "    phases = ['train', 'val', 'test'] if test_dl is not None else ['train', 'val']\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "                dl = dataloaders[train_dl]\n",
    "                dataset_size = dataset_sizes[train_dl]\n",
    "                \n",
    "            elif phase == 'val':\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dl = dataloaders[val_dl]\n",
    "                dataset_size = dataset_sizes[val_dl]\n",
    "            else:\n",
    "                model.eval()\n",
    "                dl = dataloaders[test_dl]\n",
    "                dataset_size = dataset_sizes[test_dl]\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            true_positives = 0.\n",
    "            true_negatives = 0.\n",
    "            false_positives = 0.\n",
    "            false_negatives = 0.\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dl:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.where(\n",
    "                        outputs >= 0.,\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device))\n",
    "                    target = torch.where(\n",
    "                        labels >= 0.5,\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device)\n",
    "                    )\n",
    "                    loss = criterion(outputs.view(target.shape), target)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                label_vals = torch.where(\n",
    "                    labels >= 0.5,\n",
    "                    torch.tensor([1.]).to(device),\n",
    "                    torch.tensor([0.]).to(device)\n",
    "                )\n",
    "                correct = preds.view(label_vals.shape) == label_vals.data\n",
    "                running_corrects += torch.sum(correct)\n",
    "                \n",
    "                true_positives += torch.sum(\n",
    "                    torch.where(\n",
    "                        (correct == 1.) * (label_vals == 1.),\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device))\n",
    "                )\n",
    "                true_negatives += torch.sum(\n",
    "                    torch.where(\n",
    "                        (correct == 1.) * (label_vals == 0.),\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device))\n",
    "                )\n",
    "                false_positives += torch.sum(\n",
    "                    torch.where(\n",
    "                        (correct == 0.) * (label_vals == 0.),\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device))\n",
    "                )\n",
    "                false_negatives += torch.sum(\n",
    "                    torch.where(\n",
    "                        (correct == 0.) * (label_vals == 1.),\n",
    "                        torch.tensor([1.]).to(device),\n",
    "                        torch.tensor([0.]).to(device))\n",
    "                )\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_acc = running_corrects.double() / dataset_size\n",
    "            epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "            epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "            epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "            if verbose:\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "                print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "                    true_positives.data, true_negatives.data, false_positives.data, false_negatives.data))\n",
    "            if log_file is not None:\n",
    "                log_file.write('Phase: {0}\\t'\n",
    "                               'Epoch: [{1}/{2}]\\t'\n",
    "                               'Loss: {loss_c:.4f}\\t'\n",
    "                               'Acc: {acc:.4f}\\t'\n",
    "                               'Pre: {pre:.4f}\\t'\n",
    "                               'Rec: {rec:.4f}\\t'\n",
    "                               'F1: {f1:.4f}\\t'\n",
    "                               'TP: {tp} '\n",
    "                               'TN: {tn} '\n",
    "                               'FP: {fp} '\n",
    "                               'FN: {fn}\\n'.format(\n",
    "                                   phase, epoch + 1, num_epochs, loss_c=epoch_loss,\n",
    "                                   acc=epoch_acc, pre=epoch_pre, rec=epoch_recall,\n",
    "                                   f1=epoch_f1, tp=int(true_positives.data), tn=int(true_negatives.data),\n",
    "                                   fp=int(false_positives.data), fn=int(false_negatives.data)\n",
    "                               ))\n",
    "                log_file.flush()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_f1 > best_f1:\n",
    "                best_acc = epoch_acc\n",
    "                best_f1 = epoch_f1\n",
    "                best_precision = epoch_pre\n",
    "                best_recall = epoch_recall\n",
    "                best_epoch = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'test' and best_epoch == epoch:\n",
    "                best_test_acc = epoch_acc\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    if return_best:\n",
    "        print('Best epoch: {}'.format(best_epoch))\n",
    "        print('Best val Acc: {:4f}'.format(best_acc))\n",
    "        print('Best val Pre: {:4f}'.format(best_precision))\n",
    "        print('Best val Rec: {:4f}'.format(best_recall))\n",
    "        print('Best val F1: {:4f}'.format(best_f1))\n",
    "        print('Test Acc: {:4f}'.format(best_test_acc))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_train val test\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3549 Acc: 0.8533 Pre: 0.7553 Rec: 0.7331 F1: 0.7441\n",
      "TP: 2022.0 TN: 6066.0 FP: 655.0 FN: 736.0\n",
      "val Loss: 0.1232 Acc: 0.9625 Pre: 0.9203 Rec: 0.9540 F1: 0.9368\n",
      "TP: 2631.0 TN: 6493.0 FP: 228.0 FN: 127.0\n",
      "test Loss: 0.2267 Acc: 0.9238 Pre: 0.8414 Rec: 0.9355 F1: 0.8860\n",
      "TP: 2218.0 TN: 4707.0 FP: 418.0 FN: 153.0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2512 Acc: 0.9056 Pre: 0.8369 Rec: 0.8390 F1: 0.8380\n",
      "TP: 2314.0 TN: 6270.0 FP: 451.0 FN: 444.0\n",
      "val Loss: 0.1567 Acc: 0.9456 Pre: 0.8559 Rec: 0.9775 F1: 0.9127\n",
      "TP: 2696.0 TN: 6267.0 FP: 454.0 FN: 62.0\n",
      "test Loss: 0.3208 Acc: 0.8877 Pre: 0.7547 Rec: 0.9553 F1: 0.8433\n",
      "TP: 2265.0 TN: 4389.0 FP: 736.0 FN: 106.0\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "test Loss: 0.2969 Acc: 0.8901 Pre: 0.7687 Rec: 0.9334 F1: 0.8430\n",
      "TP: 2213.0 TN: 4459.0 FP: 666.0 FN: 158.0\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2029 Acc: 0.9213 Pre: 0.8598 Rec: 0.8716 F1: 0.8657\n",
      "TP: 2404.0 TN: 6329.0 FP: 392.0 FN: 354.0\n",
      "val Loss: 0.0796 Acc: 0.9750 Pre: 0.9465 Rec: 0.9688 F1: 0.9575\n",
      "TP: 2672.0 TN: 6570.0 FP: 151.0 FN: 86.0\n",
      "test Loss: 0.2258 Acc: 0.9306 Pre: 0.8536 Rec: 0.9422 F1: 0.8957\n",
      "TP: 2234.0 TN: 4742.0 FP: 383.0 FN: 137.0\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1958 Acc: 0.9222 Pre: 0.8634 Rec: 0.8706 F1: 0.8669\n",
      "TP: 2401.0 TN: 6341.0 FP: 380.0 FN: 357.0\n",
      "val Loss: 0.0904 Acc: 0.9755 Pre: 0.9850 Rec: 0.9300 F1: 0.9567\n",
      "TP: 2565.0 TN: 6682.0 FP: 39.0 FN: 193.0\n",
      "test Loss: 0.2091 Acc: 0.9402 Pre: 0.9317 Rec: 0.8752 F1: 0.9026\n",
      "TP: 2075.0 TN: 4973.0 FP: 152.0 FN: 296.0\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1897 Acc: 0.9270 Pre: 0.8689 Rec: 0.8822 F1: 0.8755\n",
      "TP: 2433.0 TN: 6354.0 FP: 367.0 FN: 325.0\n",
      "val Loss: 0.0800 Acc: 0.9800 Pre: 0.9787 Rec: 0.9518 F1: 0.9651\n",
      "TP: 2625.0 TN: 6664.0 FP: 57.0 FN: 133.0\n",
      "test Loss: 0.1826 Acc: 0.9494 Pre: 0.9319 Rec: 0.9064 F1: 0.9190\n",
      "TP: 2149.0 TN: 4968.0 FP: 157.0 FN: 222.0\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1533 Acc: 0.9424 Pre: 0.8964 Rec: 0.9068 F1: 0.9016\n",
      "TP: 2501.0 TN: 6432.0 FP: 289.0 FN: 257.0\n",
      "val Loss: 0.0615 Acc: 0.9834 Pre: 0.9755 Rec: 0.9674 F1: 0.9714\n",
      "TP: 2668.0 TN: 6654.0 FP: 67.0 FN: 90.0\n",
      "test Loss: 0.1887 Acc: 0.9448 Pre: 0.9002 Rec: 0.9283 F1: 0.9140\n",
      "TP: 2201.0 TN: 4881.0 FP: 244.0 FN: 170.0\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1436 Acc: 0.9466 Pre: 0.9039 Rec: 0.9137 F1: 0.9088\n",
      "TP: 2520.0 TN: 6453.0 FP: 268.0 FN: 238.0\n",
      "val Loss: 0.0578 Acc: 0.9848 Pre: 0.9795 Rec: 0.9681 F1: 0.9737\n",
      "TP: 2670.0 TN: 6665.0 FP: 56.0 FN: 88.0\n",
      "test Loss: 0.1904 Acc: 0.9445 Pre: 0.9001 Rec: 0.9275 F1: 0.9136\n",
      "TP: 2199.0 TN: 4881.0 FP: 244.0 FN: 172.0\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1365 Acc: 0.9510 Pre: 0.9126 Rec: 0.9199 F1: 0.9162\n",
      "TP: 2537.0 TN: 6478.0 FP: 243.0 FN: 221.0\n",
      "val Loss: 0.0549 Acc: 0.9860 Pre: 0.9792 Rec: 0.9724 F1: 0.9758\n",
      "TP: 2682.0 TN: 6664.0 FP: 57.0 FN: 76.0\n",
      "test Loss: 0.1933 Acc: 0.9454 Pre: 0.8985 Rec: 0.9329 F1: 0.9154\n",
      "TP: 2212.0 TN: 4875.0 FP: 250.0 FN: 159.0\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1325 Acc: 0.9522 Pre: 0.9171 Rec: 0.9188 F1: 0.9179\n",
      "TP: 2534.0 TN: 6492.0 FP: 229.0 FN: 224.0\n",
      "val Loss: 0.0547 Acc: 0.9841 Pre: 0.9677 Rec: 0.9779 F1: 0.9728\n",
      "TP: 2697.0 TN: 6631.0 FP: 90.0 FN: 61.0\n",
      "test Loss: 0.2281 Acc: 0.9353 Pre: 0.8638 Rec: 0.9443 F1: 0.9023\n",
      "TP: 2239.0 TN: 4772.0 FP: 353.0 FN: 132.0\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1312 Acc: 0.9515 Pre: 0.9142 Rec: 0.9195 F1: 0.9168\n",
      "TP: 2536.0 TN: 6483.0 FP: 238.0 FN: 222.0\n",
      "val Loss: 0.0504 Acc: 0.9861 Pre: 0.9810 Rec: 0.9710 F1: 0.9759\n",
      "TP: 2678.0 TN: 6669.0 FP: 52.0 FN: 80.0\n",
      "test Loss: 0.1837 Acc: 0.9474 Pre: 0.9053 Rec: 0.9313 F1: 0.9181\n",
      "TP: 2208.0 TN: 4894.0 FP: 231.0 FN: 163.0\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1246 Acc: 0.9558 Pre: 0.9227 Rec: 0.9257 F1: 0.9242\n",
      "TP: 2553.0 TN: 6507.0 FP: 214.0 FN: 205.0\n",
      "val Loss: 0.0491 Acc: 0.9869 Pre: 0.9810 Rec: 0.9739 F1: 0.9774\n",
      "TP: 2686.0 TN: 6669.0 FP: 52.0 FN: 72.0\n",
      "test Loss: 0.1952 Acc: 0.9409 Pre: 0.8881 Rec: 0.9304 F1: 0.9088\n",
      "TP: 2206.0 TN: 4847.0 FP: 278.0 FN: 165.0\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1305 Acc: 0.9518 Pre: 0.9131 Rec: 0.9220 F1: 0.9176\n",
      "TP: 2543.0 TN: 6479.0 FP: 242.0 FN: 215.0\n",
      "val Loss: 0.0480 Acc: 0.9873 Pre: 0.9814 Rec: 0.9750 F1: 0.9782\n",
      "TP: 2689.0 TN: 6670.0 FP: 51.0 FN: 69.0\n",
      "test Loss: 0.2098 Acc: 0.9381 Pre: 0.8822 Rec: 0.9283 F1: 0.9046\n",
      "TP: 2201.0 TN: 4831.0 FP: 294.0 FN: 170.0\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1200 Acc: 0.9544 Pre: 0.9217 Rec: 0.9217 F1: 0.9217\n",
      "TP: 2542.0 TN: 6505.0 FP: 216.0 FN: 216.0\n",
      "val Loss: 0.0485 Acc: 0.9861 Pre: 0.9737 Rec: 0.9786 F1: 0.9761\n",
      "TP: 2699.0 TN: 6648.0 FP: 73.0 FN: 59.0\n",
      "test Loss: 0.2310 Acc: 0.9320 Pre: 0.8550 Rec: 0.9452 F1: 0.8978\n",
      "TP: 2241.0 TN: 4745.0 FP: 380.0 FN: 130.0\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.9613 Pre: 0.9317 Rec: 0.9355 F1: 0.9336\n",
      "TP: 2580.0 TN: 6532.0 FP: 189.0 FN: 178.0\n",
      "val Loss: 0.0480 Acc: 0.9865 Pre: 0.9768 Rec: 0.9768 F1: 0.9768\n",
      "TP: 2694.0 TN: 6657.0 FP: 64.0 FN: 64.0\n",
      "test Loss: 0.2116 Acc: 0.9373 Pre: 0.8732 Rec: 0.9380 F1: 0.9044\n",
      "TP: 2224.0 TN: 4802.0 FP: 323.0 FN: 147.0\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1233 Acc: 0.9543 Pre: 0.9213 Rec: 0.9217 F1: 0.9215\n",
      "TP: 2542.0 TN: 6504.0 FP: 217.0 FN: 216.0\n",
      "val Loss: 0.0471 Acc: 0.9866 Pre: 0.9761 Rec: 0.9779 F1: 0.9770\n",
      "TP: 2697.0 TN: 6655.0 FP: 66.0 FN: 61.0\n",
      "test Loss: 0.2194 Acc: 0.9350 Pre: 0.8651 Rec: 0.9414 F1: 0.9016\n",
      "TP: 2232.0 TN: 4777.0 FP: 348.0 FN: 139.0\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1173 Acc: 0.9545 Pre: 0.9157 Rec: 0.9293 F1: 0.9224\n",
      "TP: 2563.0 TN: 6485.0 FP: 236.0 FN: 195.0\n",
      "val Loss: 0.0461 Acc: 0.9879 Pre: 0.9842 Rec: 0.9739 F1: 0.9790\n",
      "TP: 2686.0 TN: 6678.0 FP: 43.0 FN: 72.0\n",
      "test Loss: 0.1957 Acc: 0.9422 Pre: 0.8971 Rec: 0.9232 F1: 0.9100\n",
      "TP: 2189.0 TN: 4874.0 FP: 251.0 FN: 182.0\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1212 Acc: 0.9570 Pre: 0.9257 Rec: 0.9264 F1: 0.9261\n",
      "TP: 2555.0 TN: 6516.0 FP: 205.0 FN: 203.0\n",
      "val Loss: 0.0463 Acc: 0.9869 Pre: 0.9789 Rec: 0.9761 F1: 0.9775\n",
      "TP: 2692.0 TN: 6663.0 FP: 58.0 FN: 66.0\n",
      "test Loss: 0.1999 Acc: 0.9388 Pre: 0.8803 Rec: 0.9334 F1: 0.9060\n",
      "TP: 2213.0 TN: 4824.0 FP: 301.0 FN: 158.0\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1275 Acc: 0.9545 Pre: 0.9172 Rec: 0.9275 F1: 0.9223\n",
      "TP: 2558.0 TN: 6490.0 FP: 231.0 FN: 200.0\n",
      "val Loss: 0.0459 Acc: 0.9867 Pre: 0.9789 Rec: 0.9753 F1: 0.9771\n",
      "TP: 2690.0 TN: 6663.0 FP: 58.0 FN: 68.0\n",
      "test Loss: 0.2064 Acc: 0.9382 Pre: 0.8837 Rec: 0.9266 F1: 0.9047\n",
      "TP: 2197.0 TN: 4836.0 FP: 289.0 FN: 174.0\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1189 Acc: 0.9589 Pre: 0.9253 Rec: 0.9340 F1: 0.9296\n",
      "TP: 2576.0 TN: 6513.0 FP: 208.0 FN: 182.0\n",
      "val Loss: 0.0469 Acc: 0.9867 Pre: 0.9761 Rec: 0.9782 F1: 0.9772\n",
      "TP: 2698.0 TN: 6655.0 FP: 66.0 FN: 60.0\n",
      "test Loss: 0.2134 Acc: 0.9360 Pre: 0.8706 Rec: 0.9367 F1: 0.9025\n",
      "TP: 2221.0 TN: 4795.0 FP: 330.0 FN: 150.0\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1203 Acc: 0.9552 Pre: 0.9168 Rec: 0.9304 F1: 0.9235\n",
      "TP: 2566.0 TN: 6488.0 FP: 233.0 FN: 192.0\n",
      "val Loss: 0.0454 Acc: 0.9871 Pre: 0.9803 Rec: 0.9753 F1: 0.9778\n",
      "TP: 2690.0 TN: 6667.0 FP: 54.0 FN: 68.0\n",
      "test Loss: 0.1936 Acc: 0.9405 Pre: 0.8848 Rec: 0.9334 F1: 0.9085\n",
      "TP: 2213.0 TN: 4837.0 FP: 288.0 FN: 158.0\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1218 Acc: 0.9548 Pre: 0.9164 Rec: 0.9297 F1: 0.9230\n",
      "TP: 2564.0 TN: 6487.0 FP: 234.0 FN: 194.0\n",
      "val Loss: 0.0459 Acc: 0.9867 Pre: 0.9758 Rec: 0.9786 F1: 0.9772\n",
      "TP: 2699.0 TN: 6654.0 FP: 67.0 FN: 59.0\n",
      "test Loss: 0.2171 Acc: 0.9333 Pre: 0.8616 Rec: 0.9401 F1: 0.8992\n",
      "TP: 2229.0 TN: 4767.0 FP: 358.0 FN: 142.0\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1206 Acc: 0.9569 Pre: 0.9220 Rec: 0.9304 F1: 0.9262\n",
      "TP: 2566.0 TN: 6504.0 FP: 217.0 FN: 192.0\n",
      "val Loss: 0.0453 Acc: 0.9879 Pre: 0.9835 Rec: 0.9746 F1: 0.9791\n",
      "TP: 2688.0 TN: 6676.0 FP: 45.0 FN: 70.0\n",
      "test Loss: 0.1994 Acc: 0.9401 Pre: 0.8872 Rec: 0.9287 F1: 0.9075\n",
      "TP: 2202.0 TN: 4845.0 FP: 280.0 FN: 169.0\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1199 Acc: 0.9538 Pre: 0.9128 Rec: 0.9300 F1: 0.9213\n",
      "TP: 2565.0 TN: 6476.0 FP: 245.0 FN: 193.0\n",
      "val Loss: 0.0459 Acc: 0.9870 Pre: 0.9800 Rec: 0.9753 F1: 0.9776\n",
      "TP: 2690.0 TN: 6666.0 FP: 55.0 FN: 68.0\n",
      "test Loss: 0.1993 Acc: 0.9406 Pre: 0.8877 Rec: 0.9300 F1: 0.9083\n",
      "TP: 2205.0 TN: 4846.0 FP: 279.0 FN: 166.0\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1181 Acc: 0.9555 Pre: 0.9189 Rec: 0.9289 F1: 0.9239\n",
      "TP: 2562.0 TN: 6495.0 FP: 226.0 FN: 196.0\n",
      "val Loss: 0.0464 Acc: 0.9866 Pre: 0.9806 Rec: 0.9732 F1: 0.9769\n",
      "TP: 2684.0 TN: 6668.0 FP: 53.0 FN: 74.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1975 Acc: 0.9400 Pre: 0.8912 Rec: 0.9228 F1: 0.9068\n",
      "TP: 2188.0 TN: 4858.0 FP: 267.0 FN: 183.0\n",
      "\n",
      "Training complete in 84m 5s\n",
      "val_train val test\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3494 Acc: 0.8579 Pre: 0.7580 Rec: 0.7516 F1: 0.7548\n",
      "TP: 2073.0 TN: 6059.0 FP: 662.0 FN: 685.0\n",
      "val Loss: 0.1126 Acc: 0.9693 Pre: 0.9533 Rec: 0.9405 F1: 0.9469\n",
      "TP: 2594.0 TN: 6594.0 FP: 127.0 FN: 164.0\n",
      "test Loss: 0.2164 Acc: 0.9318 Pre: 0.8796 Rec: 0.9089 F1: 0.8940\n",
      "TP: 2155.0 TN: 4830.0 FP: 295.0 FN: 216.0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2504 Acc: 0.9077 Pre: 0.8395 Rec: 0.8441 F1: 0.8418\n",
      "TP: 2328.0 TN: 6276.0 FP: 445.0 FN: 430.0\n",
      "val Loss: 0.1087 Acc: 0.9707 Pre: 0.9658 Rec: 0.9322 F1: 0.9487\n",
      "TP: 2571.0 TN: 6630.0 FP: 91.0 FN: 187.0\n",
      "test Loss: 0.1750 Acc: 0.9458 Pre: 0.9190 Rec: 0.9089 F1: 0.9139\n",
      "TP: 2155.0 TN: 4935.0 FP: 190.0 FN: 216.0\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2197 Acc: 0.9158 Pre: 0.8523 Rec: 0.8597 F1: 0.8560\n",
      "TP: 2371.0 TN: 6310.0 FP: 411.0 FN: 387.0\n",
      "val Loss: 0.1395 Acc: 0.9659 Pre: 0.9833 Rec: 0.8981 F1: 0.9388\n",
      "TP: 2477.0 TN: 6679.0 FP: 42.0 FN: 281.0\n",
      "test Loss: 0.2329 Acc: 0.9416 Pre: 0.9506 Rec: 0.8600 F1: 0.9030\n",
      "TP: 2039.0 TN: 5019.0 FP: 106.0 FN: 332.0\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1996 Acc: 0.9228 Pre: 0.8615 Rec: 0.8753 F1: 0.8683\n",
      "TP: 2414.0 TN: 6333.0 FP: 388.0 FN: 344.0\n",
      "val Loss: 0.0853 Acc: 0.9762 Pre: 0.9752 Rec: 0.9420 F1: 0.9583\n",
      "TP: 2598.0 TN: 6655.0 FP: 66.0 FN: 160.0\n",
      "test Loss: 0.1959 Acc: 0.9413 Pre: 0.9041 Rec: 0.9110 F1: 0.9076\n",
      "TP: 2160.0 TN: 4896.0 FP: 229.0 FN: 211.0\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1909 Acc: 0.9273 Pre: 0.8701 Rec: 0.8818 F1: 0.8759\n",
      "TP: 2432.0 TN: 6358.0 FP: 363.0 FN: 326.0\n",
      "val Loss: 0.0805 Acc: 0.9740 Pre: 0.9432 Rec: 0.9692 F1: 0.9560\n",
      "TP: 2673.0 TN: 6560.0 FP: 161.0 FN: 85.0\n",
      "test Loss: 0.2234 Acc: 0.9294 Pre: 0.8542 Rec: 0.9367 F1: 0.8936\n",
      "TP: 2221.0 TN: 4746.0 FP: 379.0 FN: 150.0\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1851 Acc: 0.9293 Pre: 0.8681 Rec: 0.8927 F1: 0.8802\n",
      "TP: 2462.0 TN: 6347.0 FP: 374.0 FN: 296.0\n",
      "val Loss: 0.0898 Acc: 0.9715 Pre: 0.9673 Rec: 0.9336 F1: 0.9502\n",
      "TP: 2575.0 TN: 6634.0 FP: 87.0 FN: 183.0\n",
      "test Loss: 0.2156 Acc: 0.9354 Pre: 0.9086 Rec: 0.8849 F1: 0.8966\n",
      "TP: 2098.0 TN: 4914.0 FP: 211.0 FN: 273.0\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1586 Acc: 0.9393 Pre: 0.8897 Rec: 0.9036 F1: 0.8966\n",
      "TP: 2492.0 TN: 6412.0 FP: 309.0 FN: 266.0\n",
      "val Loss: 0.0593 Acc: 0.9825 Pre: 0.9740 Rec: 0.9656 F1: 0.9698\n",
      "TP: 2663.0 TN: 6650.0 FP: 71.0 FN: 95.0\n",
      "test Loss: 0.2014 Acc: 0.9430 Pre: 0.8948 Rec: 0.9291 F1: 0.9116\n",
      "TP: 2203.0 TN: 4866.0 FP: 259.0 FN: 168.0\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1443 Acc: 0.9431 Pre: 0.8990 Rec: 0.9065 F1: 0.9027\n",
      "TP: 2500.0 TN: 6440.0 FP: 281.0 FN: 258.0\n",
      "val Loss: 0.0520 Acc: 0.9846 Pre: 0.9787 Rec: 0.9681 F1: 0.9734\n",
      "TP: 2670.0 TN: 6663.0 FP: 58.0 FN: 88.0\n",
      "test Loss: 0.1949 Acc: 0.9456 Pre: 0.8995 Rec: 0.9321 F1: 0.9155\n",
      "TP: 2210.0 TN: 4878.0 FP: 247.0 FN: 161.0\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1415 Acc: 0.9447 Pre: 0.8998 Rec: 0.9115 F1: 0.9056\n",
      "TP: 2514.0 TN: 6441.0 FP: 280.0 FN: 244.0\n",
      "val Loss: 0.0534 Acc: 0.9847 Pre: 0.9749 Rec: 0.9724 F1: 0.9737\n",
      "TP: 2682.0 TN: 6652.0 FP: 69.0 FN: 76.0\n",
      "test Loss: 0.2068 Acc: 0.9400 Pre: 0.8825 Rec: 0.9346 F1: 0.9078\n",
      "TP: 2216.0 TN: 4830.0 FP: 295.0 FN: 155.0\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1369 Acc: 0.9510 Pre: 0.9123 Rec: 0.9202 F1: 0.9162\n",
      "TP: 2538.0 TN: 6477.0 FP: 244.0 FN: 220.0\n",
      "val Loss: 0.0484 Acc: 0.9863 Pre: 0.9785 Rec: 0.9743 F1: 0.9764\n",
      "TP: 2687.0 TN: 6662.0 FP: 59.0 FN: 71.0\n",
      "test Loss: 0.1962 Acc: 0.9442 Pre: 0.8908 Rec: 0.9388 F1: 0.9142\n",
      "TP: 2226.0 TN: 4852.0 FP: 273.0 FN: 145.0\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1366 Acc: 0.9495 Pre: 0.9086 Rec: 0.9188 F1: 0.9136\n",
      "TP: 2534.0 TN: 6466.0 FP: 255.0 FN: 224.0\n",
      "val Loss: 0.0483 Acc: 0.9860 Pre: 0.9820 Rec: 0.9695 F1: 0.9757\n",
      "TP: 2674.0 TN: 6672.0 FP: 49.0 FN: 84.0\n",
      "test Loss: 0.1881 Acc: 0.9476 Pre: 0.9090 Rec: 0.9270 F1: 0.9179\n",
      "TP: 2198.0 TN: 4905.0 FP: 220.0 FN: 173.0\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1294 Acc: 0.9526 Pre: 0.9154 Rec: 0.9224 F1: 0.9189\n",
      "TP: 2544.0 TN: 6486.0 FP: 235.0 FN: 214.0\n",
      "val Loss: 0.0459 Acc: 0.9854 Pre: 0.9816 Rec: 0.9681 F1: 0.9748\n",
      "TP: 2670.0 TN: 6671.0 FP: 50.0 FN: 88.0\n",
      "test Loss: 0.1892 Acc: 0.9469 Pre: 0.9116 Rec: 0.9216 F1: 0.9165\n",
      "TP: 2185.0 TN: 4913.0 FP: 212.0 FN: 186.0\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.9555 Pre: 0.9223 Rec: 0.9249 F1: 0.9236\n",
      "TP: 2551.0 TN: 6506.0 FP: 215.0 FN: 207.0\n",
      "val Loss: 0.0454 Acc: 0.9863 Pre: 0.9845 Rec: 0.9681 F1: 0.9762\n",
      "TP: 2670.0 TN: 6679.0 FP: 42.0 FN: 88.0\n",
      "test Loss: 0.1804 Acc: 0.9492 Pre: 0.9112 Rec: 0.9300 F1: 0.9205\n",
      "TP: 2205.0 TN: 4910.0 FP: 215.0 FN: 166.0\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1247 Acc: 0.9536 Pre: 0.9163 Rec: 0.9249 F1: 0.9206\n",
      "TP: 2551.0 TN: 6488.0 FP: 233.0 FN: 207.0\n",
      "val Loss: 0.0458 Acc: 0.9862 Pre: 0.9799 Rec: 0.9724 F1: 0.9762\n",
      "TP: 2682.0 TN: 6666.0 FP: 55.0 FN: 76.0\n",
      "test Loss: 0.1867 Acc: 0.9464 Pre: 0.9020 Rec: 0.9317 F1: 0.9166\n",
      "TP: 2209.0 TN: 4885.0 FP: 240.0 FN: 162.0\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1222 Acc: 0.9526 Pre: 0.9148 Rec: 0.9231 F1: 0.9190\n",
      "TP: 2546.0 TN: 6484.0 FP: 237.0 FN: 212.0\n",
      "val Loss: 0.0455 Acc: 0.9863 Pre: 0.9761 Rec: 0.9768 F1: 0.9764\n",
      "TP: 2694.0 TN: 6655.0 FP: 66.0 FN: 64.0\n",
      "test Loss: 0.2030 Acc: 0.9412 Pre: 0.8820 Rec: 0.9397 F1: 0.9099\n",
      "TP: 2228.0 TN: 4827.0 FP: 298.0 FN: 143.0\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9544 Pre: 0.9208 Rec: 0.9228 F1: 0.9218\n",
      "TP: 2545.0 TN: 6502.0 FP: 219.0 FN: 213.0\n",
      "val Loss: 0.0434 Acc: 0.9868 Pre: 0.9786 Rec: 0.9761 F1: 0.9773\n",
      "TP: 2692.0 TN: 6662.0 FP: 59.0 FN: 66.0\n",
      "test Loss: 0.2014 Acc: 0.9417 Pre: 0.8843 Rec: 0.9384 F1: 0.9106\n",
      "TP: 2225.0 TN: 4834.0 FP: 291.0 FN: 146.0\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1262 Acc: 0.9544 Pre: 0.9151 Rec: 0.9297 F1: 0.9223\n",
      "TP: 2564.0 TN: 6483.0 FP: 238.0 FN: 194.0\n",
      "val Loss: 0.0428 Acc: 0.9867 Pre: 0.9831 Rec: 0.9710 F1: 0.9770\n",
      "TP: 2678.0 TN: 6675.0 FP: 46.0 FN: 80.0\n",
      "test Loss: 0.1849 Acc: 0.9466 Pre: 0.9024 Rec: 0.9321 F1: 0.9170\n",
      "TP: 2210.0 TN: 4886.0 FP: 239.0 FN: 161.0\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1242 Acc: 0.9552 Pre: 0.9222 Rec: 0.9239 F1: 0.9230\n",
      "TP: 2548.0 TN: 6506.0 FP: 215.0 FN: 210.0\n",
      "val Loss: 0.0467 Acc: 0.9860 Pre: 0.9730 Rec: 0.9790 F1: 0.9760\n",
      "TP: 2700.0 TN: 6646.0 FP: 75.0 FN: 58.0\n",
      "test Loss: 0.2222 Acc: 0.9344 Pre: 0.8590 Rec: 0.9481 F1: 0.9014\n",
      "TP: 2248.0 TN: 4756.0 FP: 369.0 FN: 123.0\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1175 Acc: 0.9553 Pre: 0.9195 Rec: 0.9275 F1: 0.9235\n",
      "TP: 2558.0 TN: 6497.0 FP: 224.0 FN: 200.0\n",
      "val Loss: 0.0461 Acc: 0.9862 Pre: 0.9710 Rec: 0.9819 F1: 0.9764\n",
      "TP: 2708.0 TN: 6640.0 FP: 81.0 FN: 50.0\n",
      "test Loss: 0.2227 Acc: 0.9338 Pre: 0.8566 Rec: 0.9498 F1: 0.9008\n",
      "TP: 2252.0 TN: 4748.0 FP: 377.0 FN: 119.0\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1240 Acc: 0.9523 Pre: 0.9162 Rec: 0.9202 F1: 0.9182\n",
      "TP: 2538.0 TN: 6489.0 FP: 232.0 FN: 220.0\n",
      "val Loss: 0.0439 Acc: 0.9871 Pre: 0.9779 Rec: 0.9779 F1: 0.9779\n",
      "TP: 2697.0 TN: 6660.0 FP: 61.0 FN: 61.0\n",
      "test Loss: 0.1932 Acc: 0.9421 Pre: 0.8845 Rec: 0.9397 F1: 0.9112\n",
      "TP: 2228.0 TN: 4834.0 FP: 291.0 FN: 143.0\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1223 Acc: 0.9550 Pre: 0.9185 Rec: 0.9275 F1: 0.9230\n",
      "TP: 2558.0 TN: 6494.0 FP: 227.0 FN: 200.0\n",
      "val Loss: 0.0433 Acc: 0.9866 Pre: 0.9771 Rec: 0.9768 F1: 0.9770\n",
      "TP: 2694.0 TN: 6658.0 FP: 63.0 FN: 64.0\n",
      "test Loss: 0.2106 Acc: 0.9396 Pre: 0.8779 Rec: 0.9397 F1: 0.9077\n",
      "TP: 2228.0 TN: 4815.0 FP: 310.0 FN: 143.0\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1235 Acc: 0.9552 Pre: 0.9177 Rec: 0.9293 F1: 0.9234\n",
      "TP: 2563.0 TN: 6491.0 FP: 230.0 FN: 195.0\n",
      "val Loss: 0.0468 Acc: 0.9855 Pre: 0.9743 Rec: 0.9761 F1: 0.9752\n",
      "TP: 2692.0 TN: 6650.0 FP: 71.0 FN: 66.0\n",
      "test Loss: 0.2104 Acc: 0.9380 Pre: 0.8746 Rec: 0.9384 F1: 0.9054\n",
      "TP: 2225.0 TN: 4806.0 FP: 319.0 FN: 146.0\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9540 Pre: 0.9167 Rec: 0.9260 F1: 0.9214\n",
      "TP: 2554.0 TN: 6489.0 FP: 232.0 FN: 204.0\n",
      "val Loss: 0.0432 Acc: 0.9866 Pre: 0.9817 Rec: 0.9721 F1: 0.9769\n",
      "TP: 2681.0 TN: 6671.0 FP: 50.0 FN: 77.0\n",
      "test Loss: 0.1951 Acc: 0.9434 Pre: 0.8933 Rec: 0.9325 F1: 0.9125\n",
      "TP: 2211.0 TN: 4861.0 FP: 264.0 FN: 160.0\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1280 Acc: 0.9524 Pre: 0.9190 Rec: 0.9173 F1: 0.9182\n",
      "TP: 2530.0 TN: 6498.0 FP: 223.0 FN: 228.0\n",
      "val Loss: 0.0474 Acc: 0.9864 Pre: 0.9723 Rec: 0.9811 F1: 0.9767\n",
      "TP: 2706.0 TN: 6644.0 FP: 77.0 FN: 52.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2070 Acc: 0.9386 Pre: 0.8699 Rec: 0.9477 F1: 0.9071\n",
      "TP: 2247.0 TN: 4789.0 FP: 336.0 FN: 124.0\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9525 Pre: 0.9101 Rec: 0.9286 F1: 0.9192\n",
      "TP: 2561.0 TN: 6468.0 FP: 253.0 FN: 197.0\n",
      "val Loss: 0.0453 Acc: 0.9855 Pre: 0.9859 Rec: 0.9641 F1: 0.9749\n",
      "TP: 2659.0 TN: 6683.0 FP: 38.0 FN: 99.0\n",
      "test Loss: 0.1870 Acc: 0.9506 Pre: 0.9216 Rec: 0.9224 F1: 0.9220\n",
      "TP: 2187.0 TN: 4939.0 FP: 186.0 FN: 184.0\n",
      "\n",
      "Training complete in 84m 20s\n",
      "val_train val test\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3447 Acc: 0.8602 Pre: 0.7671 Rec: 0.7462 F1: 0.7565\n",
      "TP: 2058.0 TN: 6096.0 FP: 625.0 FN: 700.0\n",
      "val Loss: 0.1445 Acc: 0.9570 Pre: 0.9454 Rec: 0.9043 F1: 0.9244\n",
      "TP: 2494.0 TN: 6577.0 FP: 144.0 FN: 264.0\n",
      "test Loss: 0.2167 Acc: 0.9301 Pre: 0.9020 Rec: 0.8739 F1: 0.8877\n",
      "TP: 2072.0 TN: 4900.0 FP: 225.0 FN: 299.0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2632 Acc: 0.8977 Pre: 0.8204 Rec: 0.8299 F1: 0.8252\n",
      "TP: 2289.0 TN: 6220.0 FP: 501.0 FN: 469.0\n",
      "val Loss: 0.0981 Acc: 0.9721 Pre: 0.9531 Rec: 0.9511 F1: 0.9521\n",
      "TP: 2623.0 TN: 6592.0 FP: 129.0 FN: 135.0\n",
      "test Loss: 0.2059 Acc: 0.9297 Pre: 0.8718 Rec: 0.9119 F1: 0.8914\n",
      "TP: 2162.0 TN: 4807.0 FP: 318.0 FN: 209.0\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2180 Acc: 0.9173 Pre: 0.8525 Rec: 0.8655 F1: 0.8589\n",
      "TP: 2387.0 TN: 6308.0 FP: 413.0 FN: 371.0\n",
      "val Loss: 0.0956 Acc: 0.9730 Pre: 0.9503 Rec: 0.9572 F1: 0.9538\n",
      "TP: 2640.0 TN: 6583.0 FP: 138.0 FN: 118.0\n",
      "test Loss: 0.2534 Acc: 0.9226 Pre: 0.8388 Rec: 0.9350 F1: 0.8843\n",
      "TP: 2217.0 TN: 4699.0 FP: 426.0 FN: 154.0\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.1992 Acc: 0.9229 Pre: 0.8647 Rec: 0.8713 F1: 0.8680\n",
      "TP: 2403.0 TN: 6345.0 FP: 376.0 FN: 355.0\n",
      "val Loss: 0.1311 Acc: 0.9531 Pre: 0.8759 Rec: 0.9772 F1: 0.9237\n",
      "TP: 2695.0 TN: 6339.0 FP: 382.0 FN: 63.0\n",
      "test Loss: 0.2712 Acc: 0.9039 Pre: 0.7846 Rec: 0.9599 F1: 0.8634\n",
      "TP: 2276.0 TN: 4500.0 FP: 625.0 FN: 95.0\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1948 Acc: 0.9268 Pre: 0.8683 Rec: 0.8822 F1: 0.8752\n",
      "TP: 2433.0 TN: 6352.0 FP: 369.0 FN: 325.0\n",
      "val Loss: 0.0796 Acc: 0.9770 Pre: 0.9595 Rec: 0.9616 F1: 0.9605\n",
      "TP: 2652.0 TN: 6609.0 FP: 112.0 FN: 106.0\n",
      "test Loss: 0.1941 Acc: 0.9434 Pre: 0.8946 Rec: 0.9308 F1: 0.9124\n",
      "TP: 2207.0 TN: 4865.0 FP: 260.0 FN: 164.0\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1824 Acc: 0.9331 Pre: 0.8815 Rec: 0.8898 F1: 0.8856\n",
      "TP: 2454.0 TN: 6391.0 FP: 330.0 FN: 304.0\n",
      "val Loss: 0.0715 Acc: 0.9793 Pre: 0.9539 Rec: 0.9761 F1: 0.9649\n",
      "TP: 2692.0 TN: 6591.0 FP: 130.0 FN: 66.0\n",
      "test Loss: 0.2235 Acc: 0.9377 Pre: 0.8684 Rec: 0.9464 F1: 0.9058\n",
      "TP: 2244.0 TN: 4785.0 FP: 340.0 FN: 127.0\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1489 Acc: 0.9447 Pre: 0.8981 Rec: 0.9137 F1: 0.9058\n",
      "TP: 2520.0 TN: 6435.0 FP: 286.0 FN: 238.0\n",
      "val Loss: 0.0637 Acc: 0.9819 Pre: 0.9615 Rec: 0.9768 F1: 0.9691\n",
      "TP: 2694.0 TN: 6613.0 FP: 108.0 FN: 64.0\n",
      "test Loss: 0.1967 Acc: 0.9426 Pre: 0.8807 Rec: 0.9469 F1: 0.9126\n",
      "TP: 2245.0 TN: 4821.0 FP: 304.0 FN: 126.0\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1502 Acc: 0.9428 Pre: 0.8963 Rec: 0.9086 F1: 0.9024\n",
      "TP: 2506.0 TN: 6431.0 FP: 290.0 FN: 252.0\n",
      "val Loss: 0.0576 Acc: 0.9843 Pre: 0.9708 Rec: 0.9753 F1: 0.9731\n",
      "TP: 2690.0 TN: 6640.0 FP: 81.0 FN: 68.0\n",
      "test Loss: 0.1858 Acc: 0.9474 Pre: 0.9030 Rec: 0.9342 F1: 0.9183\n",
      "TP: 2215.0 TN: 4887.0 FP: 238.0 FN: 156.0\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1437 Acc: 0.9471 Pre: 0.9035 Rec: 0.9162 F1: 0.9098\n",
      "TP: 2527.0 TN: 6451.0 FP: 270.0 FN: 231.0\n",
      "val Loss: 0.0577 Acc: 0.9850 Pre: 0.9722 Rec: 0.9764 F1: 0.9743\n",
      "TP: 2693.0 TN: 6644.0 FP: 77.0 FN: 65.0\n",
      "test Loss: 0.1872 Acc: 0.9453 Pre: 0.9000 Rec: 0.9304 F1: 0.9150\n",
      "TP: 2206.0 TN: 4880.0 FP: 245.0 FN: 165.0\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1362 Acc: 0.9516 Pre: 0.9130 Rec: 0.9213 F1: 0.9172\n",
      "TP: 2541.0 TN: 6479.0 FP: 242.0 FN: 217.0\n",
      "val Loss: 0.0531 Acc: 0.9859 Pre: 0.9816 Rec: 0.9695 F1: 0.9756\n",
      "TP: 2674.0 TN: 6671.0 FP: 50.0 FN: 84.0\n",
      "test Loss: 0.1819 Acc: 0.9508 Pre: 0.9252 Rec: 0.9186 F1: 0.9219\n",
      "TP: 2178.0 TN: 4949.0 FP: 176.0 FN: 193.0\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1269 Acc: 0.9532 Pre: 0.9153 Rec: 0.9246 F1: 0.9199\n",
      "TP: 2550.0 TN: 6485.0 FP: 236.0 FN: 208.0\n",
      "val Loss: 0.0488 Acc: 0.9860 Pre: 0.9761 Rec: 0.9757 F1: 0.9759\n",
      "TP: 2691.0 TN: 6655.0 FP: 66.0 FN: 67.0\n",
      "test Loss: 0.1873 Acc: 0.9506 Pre: 0.9079 Rec: 0.9393 F1: 0.9233\n",
      "TP: 2227.0 TN: 4899.0 FP: 226.0 FN: 144.0\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9513 Pre: 0.9127 Rec: 0.9206 F1: 0.9166\n",
      "TP: 2539.0 TN: 6478.0 FP: 243.0 FN: 219.0\n",
      "val Loss: 0.0498 Acc: 0.9865 Pre: 0.9820 Rec: 0.9714 F1: 0.9767\n",
      "TP: 2679.0 TN: 6672.0 FP: 49.0 FN: 79.0\n",
      "test Loss: 0.1814 Acc: 0.9500 Pre: 0.9222 Rec: 0.9194 F1: 0.9208\n",
      "TP: 2180.0 TN: 4941.0 FP: 184.0 FN: 191.0\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1244 Acc: 0.9552 Pre: 0.9180 Rec: 0.9289 F1: 0.9234\n",
      "TP: 2562.0 TN: 6492.0 FP: 229.0 FN: 196.0\n",
      "val Loss: 0.0485 Acc: 0.9861 Pre: 0.9782 Rec: 0.9739 F1: 0.9760\n",
      "TP: 2686.0 TN: 6661.0 FP: 60.0 FN: 72.0\n",
      "test Loss: 0.1905 Acc: 0.9468 Pre: 0.9061 Rec: 0.9279 F1: 0.9169\n",
      "TP: 2200.0 TN: 4897.0 FP: 228.0 FN: 171.0\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1252 Acc: 0.9550 Pre: 0.9203 Rec: 0.9253 F1: 0.9228\n",
      "TP: 2552.0 TN: 6500.0 FP: 221.0 FN: 206.0\n",
      "val Loss: 0.0489 Acc: 0.9860 Pre: 0.9737 Rec: 0.9782 F1: 0.9759\n",
      "TP: 2698.0 TN: 6648.0 FP: 73.0 FN: 60.0\n",
      "test Loss: 0.1883 Acc: 0.9461 Pre: 0.8961 Rec: 0.9384 F1: 0.9168\n",
      "TP: 2225.0 TN: 4867.0 FP: 258.0 FN: 146.0\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1180 Acc: 0.9553 Pre: 0.9171 Rec: 0.9304 F1: 0.9237\n",
      "TP: 2566.0 TN: 6489.0 FP: 232.0 FN: 192.0\n",
      "val Loss: 0.0478 Acc: 0.9864 Pre: 0.9764 Rec: 0.9768 F1: 0.9766\n",
      "TP: 2694.0 TN: 6656.0 FP: 65.0 FN: 64.0\n",
      "test Loss: 0.1880 Acc: 0.9474 Pre: 0.9026 Rec: 0.9346 F1: 0.9184\n",
      "TP: 2216.0 TN: 4886.0 FP: 239.0 FN: 155.0\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9571 Pre: 0.9258 Rec: 0.9268 F1: 0.9263\n",
      "TP: 2556.0 TN: 6516.0 FP: 205.0 FN: 202.0\n",
      "val Loss: 0.0470 Acc: 0.9870 Pre: 0.9803 Rec: 0.9750 F1: 0.9776\n",
      "TP: 2689.0 TN: 6667.0 FP: 54.0 FN: 69.0\n",
      "test Loss: 0.1810 Acc: 0.9500 Pre: 0.9158 Rec: 0.9270 F1: 0.9214\n",
      "TP: 2198.0 TN: 4923.0 FP: 202.0 FN: 173.0\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1182 Acc: 0.9579 Pre: 0.9238 Rec: 0.9322 F1: 0.9280\n",
      "TP: 2571.0 TN: 6509.0 FP: 212.0 FN: 187.0\n",
      "val Loss: 0.0472 Acc: 0.9867 Pre: 0.9768 Rec: 0.9775 F1: 0.9772\n",
      "TP: 2696.0 TN: 6657.0 FP: 64.0 FN: 62.0\n",
      "test Loss: 0.1845 Acc: 0.9480 Pre: 0.9028 Rec: 0.9363 F1: 0.9193\n",
      "TP: 2220.0 TN: 4886.0 FP: 239.0 FN: 151.0\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1266 Acc: 0.9523 Pre: 0.9136 Rec: 0.9235 F1: 0.9185\n",
      "TP: 2547.0 TN: 6480.0 FP: 241.0 FN: 211.0\n",
      "val Loss: 0.0463 Acc: 0.9866 Pre: 0.9758 Rec: 0.9782 F1: 0.9770\n",
      "TP: 2698.0 TN: 6654.0 FP: 67.0 FN: 60.0\n",
      "test Loss: 0.1918 Acc: 0.9460 Pre: 0.8954 Rec: 0.9388 F1: 0.9166\n",
      "TP: 2226.0 TN: 4865.0 FP: 260.0 FN: 145.0\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1261 Acc: 0.9554 Pre: 0.9201 Rec: 0.9271 F1: 0.9236\n",
      "TP: 2557.0 TN: 6499.0 FP: 222.0 FN: 201.0\n",
      "val Loss: 0.0474 Acc: 0.9855 Pre: 0.9723 Rec: 0.9782 F1: 0.9752\n",
      "TP: 2698.0 TN: 6644.0 FP: 77.0 FN: 60.0\n",
      "test Loss: 0.1946 Acc: 0.9441 Pre: 0.8898 Rec: 0.9397 F1: 0.9141\n",
      "TP: 2228.0 TN: 4849.0 FP: 276.0 FN: 143.0\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1194 Acc: 0.9542 Pre: 0.9174 Rec: 0.9260 F1: 0.9217\n",
      "TP: 2554.0 TN: 6491.0 FP: 230.0 FN: 204.0\n",
      "val Loss: 0.0461 Acc: 0.9869 Pre: 0.9775 Rec: 0.9775 F1: 0.9775\n",
      "TP: 2696.0 TN: 6659.0 FP: 62.0 FN: 62.0\n",
      "test Loss: 0.1859 Acc: 0.9480 Pre: 0.9028 Rec: 0.9363 F1: 0.9193\n",
      "TP: 2220.0 TN: 4886.0 FP: 239.0 FN: 151.0\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9560 Pre: 0.9158 Rec: 0.9347 F1: 0.9252\n",
      "TP: 2578.0 TN: 6484.0 FP: 237.0 FN: 180.0\n",
      "val Loss: 0.0470 Acc: 0.9867 Pre: 0.9792 Rec: 0.9750 F1: 0.9771\n",
      "TP: 2689.0 TN: 6664.0 FP: 57.0 FN: 69.0\n",
      "test Loss: 0.1858 Acc: 0.9478 Pre: 0.9091 Rec: 0.9279 F1: 0.9184\n",
      "TP: 2200.0 TN: 4905.0 FP: 220.0 FN: 171.0\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9553 Pre: 0.9222 Rec: 0.9242 F1: 0.9232\n",
      "TP: 2549.0 TN: 6506.0 FP: 215.0 FN: 209.0\n",
      "val Loss: 0.0456 Acc: 0.9873 Pre: 0.9789 Rec: 0.9775 F1: 0.9782\n",
      "TP: 2696.0 TN: 6663.0 FP: 58.0 FN: 62.0\n",
      "test Loss: 0.1903 Acc: 0.9484 Pre: 0.9066 Rec: 0.9329 F1: 0.9196\n",
      "TP: 2212.0 TN: 4897.0 FP: 228.0 FN: 159.0\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1307 Acc: 0.9514 Pre: 0.9083 Rec: 0.9264 F1: 0.9173\n",
      "TP: 2555.0 TN: 6463.0 FP: 258.0 FN: 203.0\n",
      "val Loss: 0.0471 Acc: 0.9853 Pre: 0.9705 Rec: 0.9793 F1: 0.9749\n",
      "TP: 2701.0 TN: 6639.0 FP: 82.0 FN: 57.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1928 Acc: 0.9424 Pre: 0.8824 Rec: 0.9435 F1: 0.9119\n",
      "TP: 2237.0 TN: 4827.0 FP: 298.0 FN: 134.0\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1253 Acc: 0.9537 Pre: 0.9181 Rec: 0.9231 F1: 0.9206\n",
      "TP: 2546.0 TN: 6494.0 FP: 227.0 FN: 212.0\n",
      "val Loss: 0.0464 Acc: 0.9869 Pre: 0.9758 Rec: 0.9793 F1: 0.9776\n",
      "TP: 2701.0 TN: 6654.0 FP: 67.0 FN: 57.0\n",
      "test Loss: 0.1863 Acc: 0.9462 Pre: 0.8974 Rec: 0.9372 F1: 0.9169\n",
      "TP: 2222.0 TN: 4871.0 FP: 254.0 FN: 149.0\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1282 Acc: 0.9504 Pre: 0.9124 Rec: 0.9177 F1: 0.9150\n",
      "TP: 2531.0 TN: 6478.0 FP: 243.0 FN: 227.0\n",
      "val Loss: 0.0487 Acc: 0.9855 Pre: 0.9723 Rec: 0.9782 F1: 0.9752\n",
      "TP: 2698.0 TN: 6644.0 FP: 77.0 FN: 60.0\n",
      "test Loss: 0.1877 Acc: 0.9452 Pre: 0.8911 Rec: 0.9418 F1: 0.9157\n",
      "TP: 2233.0 TN: 4852.0 FP: 273.0 FN: 138.0\n",
      "\n",
      "Training complete in 84m 30s\n",
      "val_train val test\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3524 Acc: 0.8558 Pre: 0.7577 Rec: 0.7415 F1: 0.7495\n",
      "TP: 2045.0 TN: 6067.0 FP: 654.0 FN: 713.0\n",
      "val Loss: 0.1118 Acc: 0.9701 Pre: 0.9515 Rec: 0.9456 F1: 0.9485\n",
      "TP: 2608.0 TN: 6588.0 FP: 133.0 FN: 150.0\n",
      "test Loss: 0.2087 Acc: 0.9320 Pre: 0.8809 Rec: 0.9076 F1: 0.8941\n",
      "TP: 2152.0 TN: 4834.0 FP: 291.0 FN: 219.0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2556 Acc: 0.9027 Pre: 0.8324 Rec: 0.8336 F1: 0.8330\n",
      "TP: 2299.0 TN: 6258.0 FP: 463.0 FN: 459.0\n",
      "val Loss: 0.1088 Acc: 0.9636 Pre: 0.9202 Rec: 0.9579 F1: 0.9387\n",
      "TP: 2642.0 TN: 6492.0 FP: 229.0 FN: 116.0\n",
      "test Loss: 0.2054 Acc: 0.9234 Pre: 0.8444 Rec: 0.9291 F1: 0.8847\n",
      "TP: 2203.0 TN: 4719.0 FP: 406.0 FN: 168.0\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2230 Acc: 0.9162 Pre: 0.8532 Rec: 0.8600 F1: 0.8566\n",
      "TP: 2372.0 TN: 6313.0 FP: 408.0 FN: 386.0\n",
      "val Loss: 0.1225 Acc: 0.9666 Pre: 0.9788 Rec: 0.9046 F1: 0.9403\n",
      "TP: 2495.0 TN: 6667.0 FP: 54.0 FN: 263.0\n",
      "test Loss: 0.2287 Acc: 0.9366 Pre: 0.9418 Rec: 0.8524 F1: 0.8948\n",
      "TP: 2021.0 TN: 5000.0 FP: 125.0 FN: 350.0\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2043 Acc: 0.9221 Pre: 0.8641 Rec: 0.8691 F1: 0.8666\n",
      "TP: 2397.0 TN: 6344.0 FP: 377.0 FN: 361.0\n",
      "val Loss: 0.0838 Acc: 0.9767 Pre: 0.9717 Rec: 0.9474 F1: 0.9594\n",
      "TP: 2613.0 TN: 6645.0 FP: 76.0 FN: 145.0\n",
      "test Loss: 0.1976 Acc: 0.9406 Pre: 0.9137 Rec: 0.8971 F1: 0.9053\n",
      "TP: 2127.0 TN: 4924.0 FP: 201.0 FN: 244.0\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1953 Acc: 0.9256 Pre: 0.8686 Rec: 0.8771 F1: 0.8728\n",
      "TP: 2419.0 TN: 6355.0 FP: 366.0 FN: 339.0\n",
      "val Loss: 0.0787 Acc: 0.9765 Pre: 0.9634 Rec: 0.9554 F1: 0.9594\n",
      "TP: 2635.0 TN: 6621.0 FP: 100.0 FN: 123.0\n",
      "test Loss: 0.2206 Acc: 0.9322 Pre: 0.8719 Rec: 0.9211 F1: 0.8958\n",
      "TP: 2184.0 TN: 4804.0 FP: 321.0 FN: 187.0\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1854 Acc: 0.9304 Pre: 0.8768 Rec: 0.8851 F1: 0.8809\n",
      "TP: 2441.0 TN: 6378.0 FP: 343.0 FN: 317.0\n",
      "val Loss: 0.0704 Acc: 0.9785 Pre: 0.9678 Rec: 0.9579 F1: 0.9628\n",
      "TP: 2642.0 TN: 6633.0 FP: 88.0 FN: 116.0\n",
      "test Loss: 0.1744 Acc: 0.9462 Pre: 0.9069 Rec: 0.9249 F1: 0.9158\n",
      "TP: 2193.0 TN: 4900.0 FP: 225.0 FN: 178.0\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1482 Acc: 0.9456 Pre: 0.9032 Rec: 0.9104 F1: 0.9068\n",
      "TP: 2511.0 TN: 6452.0 FP: 269.0 FN: 247.0\n",
      "val Loss: 0.0624 Acc: 0.9828 Pre: 0.9800 Rec: 0.9605 F1: 0.9702\n",
      "TP: 2649.0 TN: 6667.0 FP: 54.0 FN: 109.0\n",
      "test Loss: 0.1798 Acc: 0.9480 Pre: 0.9132 Rec: 0.9232 F1: 0.9182\n",
      "TP: 2189.0 TN: 4917.0 FP: 208.0 FN: 182.0\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1478 Acc: 0.9428 Pre: 0.9000 Rec: 0.9039 F1: 0.9020\n",
      "TP: 2493.0 TN: 6444.0 FP: 277.0 FN: 265.0\n",
      "val Loss: 0.0576 Acc: 0.9826 Pre: 0.9716 Rec: 0.9685 F1: 0.9700\n",
      "TP: 2671.0 TN: 6643.0 FP: 78.0 FN: 87.0\n",
      "test Loss: 0.1991 Acc: 0.9413 Pre: 0.8845 Rec: 0.9367 F1: 0.9099\n",
      "TP: 2221.0 TN: 4835.0 FP: 290.0 FN: 150.0\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1354 Acc: 0.9499 Pre: 0.9105 Rec: 0.9181 F1: 0.9142\n",
      "TP: 2532.0 TN: 6472.0 FP: 249.0 FN: 226.0\n",
      "val Loss: 0.0543 Acc: 0.9826 Pre: 0.9730 Rec: 0.9670 F1: 0.9700\n",
      "TP: 2667.0 TN: 6647.0 FP: 74.0 FN: 91.0\n",
      "test Loss: 0.1926 Acc: 0.9410 Pre: 0.8835 Rec: 0.9372 F1: 0.9095\n",
      "TP: 2222.0 TN: 4832.0 FP: 293.0 FN: 149.0\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1280 Acc: 0.9527 Pre: 0.9173 Rec: 0.9206 F1: 0.9189\n",
      "TP: 2539.0 TN: 6492.0 FP: 229.0 FN: 219.0\n",
      "val Loss: 0.0567 Acc: 0.9831 Pre: 0.9847 Rec: 0.9569 F1: 0.9706\n",
      "TP: 2639.0 TN: 6680.0 FP: 41.0 FN: 119.0\n",
      "test Loss: 0.1918 Acc: 0.9474 Pre: 0.9194 Rec: 0.9140 F1: 0.9167\n",
      "TP: 2167.0 TN: 4935.0 FP: 190.0 FN: 204.0\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1339 Acc: 0.9512 Pre: 0.9150 Rec: 0.9173 F1: 0.9162\n",
      "TP: 2530.0 TN: 6486.0 FP: 235.0 FN: 228.0\n",
      "val Loss: 0.0551 Acc: 0.9835 Pre: 0.9683 Rec: 0.9753 F1: 0.9718\n",
      "TP: 2690.0 TN: 6633.0 FP: 88.0 FN: 68.0\n",
      "test Loss: 0.2387 Acc: 0.9338 Pre: 0.8646 Rec: 0.9376 F1: 0.8996\n",
      "TP: 2223.0 TN: 4777.0 FP: 348.0 FN: 148.0\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1309 Acc: 0.9515 Pre: 0.9127 Rec: 0.9213 F1: 0.9170\n",
      "TP: 2541.0 TN: 6478.0 FP: 243.0 FN: 217.0\n",
      "val Loss: 0.0491 Acc: 0.9864 Pre: 0.9820 Rec: 0.9710 F1: 0.9765\n",
      "TP: 2678.0 TN: 6672.0 FP: 49.0 FN: 80.0\n",
      "test Loss: 0.1854 Acc: 0.9444 Pre: 0.9031 Rec: 0.9232 F1: 0.9130\n",
      "TP: 2189.0 TN: 4890.0 FP: 235.0 FN: 182.0\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1322 Acc: 0.9502 Pre: 0.9138 Rec: 0.9152 F1: 0.9145\n",
      "TP: 2524.0 TN: 6483.0 FP: 238.0 FN: 234.0\n",
      "val Loss: 0.0582 Acc: 0.9828 Pre: 0.9642 Rec: 0.9772 F1: 0.9706\n",
      "TP: 2695.0 TN: 6621.0 FP: 100.0 FN: 63.0\n",
      "test Loss: 0.2179 Acc: 0.9340 Pre: 0.8647 Rec: 0.9380 F1: 0.8999\n",
      "TP: 2224.0 TN: 4777.0 FP: 348.0 FN: 147.0\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1277 Acc: 0.9540 Pre: 0.9149 Rec: 0.9282 F1: 0.9215\n",
      "TP: 2560.0 TN: 6483.0 FP: 238.0 FN: 198.0\n",
      "val Loss: 0.0505 Acc: 0.9851 Pre: 0.9788 Rec: 0.9699 F1: 0.9743\n",
      "TP: 2675.0 TN: 6663.0 FP: 58.0 FN: 83.0\n",
      "test Loss: 0.1940 Acc: 0.9421 Pre: 0.8994 Rec: 0.9199 F1: 0.9095\n",
      "TP: 2181.0 TN: 4881.0 FP: 244.0 FN: 190.0\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.1304 Acc: 0.9517 Pre: 0.9146 Rec: 0.9199 F1: 0.9172\n",
      "TP: 2537.0 TN: 6484.0 FP: 237.0 FN: 221.0\n",
      "val Loss: 0.0504 Acc: 0.9850 Pre: 0.9852 Rec: 0.9630 F1: 0.9740\n",
      "TP: 2656.0 TN: 6681.0 FP: 40.0 FN: 102.0\n",
      "test Loss: 0.1914 Acc: 0.9458 Pre: 0.9172 Rec: 0.9110 F1: 0.9141\n",
      "TP: 2160.0 TN: 4930.0 FP: 195.0 FN: 211.0\n",
      "\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.1263 Acc: 0.9548 Pre: 0.9182 Rec: 0.9275 F1: 0.9228\n",
      "TP: 2558.0 TN: 6493.0 FP: 228.0 FN: 200.0\n",
      "val Loss: 0.0473 Acc: 0.9864 Pre: 0.9806 Rec: 0.9724 F1: 0.9765\n",
      "TP: 2682.0 TN: 6668.0 FP: 53.0 FN: 76.0\n",
      "test Loss: 0.2022 Acc: 0.9430 Pre: 0.8942 Rec: 0.9300 F1: 0.9117\n",
      "TP: 2205.0 TN: 4864.0 FP: 261.0 FN: 166.0\n",
      "\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.1164 Acc: 0.9571 Pre: 0.9230 Rec: 0.9300 F1: 0.9265\n",
      "TP: 2565.0 TN: 6507.0 FP: 214.0 FN: 193.0\n",
      "val Loss: 0.0485 Acc: 0.9862 Pre: 0.9859 Rec: 0.9663 F1: 0.9760\n",
      "TP: 2665.0 TN: 6683.0 FP: 38.0 FN: 93.0\n",
      "test Loss: 0.1848 Acc: 0.9460 Pre: 0.9141 Rec: 0.9152 F1: 0.9146\n",
      "TP: 2170.0 TN: 4921.0 FP: 204.0 FN: 201.0\n",
      "\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.1212 Acc: 0.9576 Pre: 0.9231 Rec: 0.9318 F1: 0.9275\n",
      "TP: 2570.0 TN: 6507.0 FP: 214.0 FN: 188.0\n",
      "val Loss: 0.0474 Acc: 0.9867 Pre: 0.9870 Rec: 0.9670 F1: 0.9769\n",
      "TP: 2667.0 TN: 6686.0 FP: 35.0 FN: 91.0\n",
      "test Loss: 0.1861 Acc: 0.9476 Pre: 0.9159 Rec: 0.9186 F1: 0.9172\n",
      "TP: 2178.0 TN: 4925.0 FP: 200.0 FN: 193.0\n",
      "\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.1251 Acc: 0.9550 Pre: 0.9212 Rec: 0.9242 F1: 0.9227\n",
      "TP: 2549.0 TN: 6503.0 FP: 218.0 FN: 209.0\n",
      "val Loss: 0.0468 Acc: 0.9865 Pre: 0.9810 Rec: 0.9724 F1: 0.9767\n",
      "TP: 2682.0 TN: 6669.0 FP: 52.0 FN: 76.0\n",
      "test Loss: 0.1910 Acc: 0.9454 Pre: 0.9037 Rec: 0.9262 F1: 0.9148\n",
      "TP: 2196.0 TN: 4891.0 FP: 234.0 FN: 175.0\n",
      "\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.1230 Acc: 0.9566 Pre: 0.9235 Rec: 0.9278 F1: 0.9257\n",
      "TP: 2559.0 TN: 6509.0 FP: 212.0 FN: 199.0\n",
      "val Loss: 0.0485 Acc: 0.9865 Pre: 0.9792 Rec: 0.9743 F1: 0.9767\n",
      "TP: 2687.0 TN: 6664.0 FP: 57.0 FN: 71.0\n",
      "test Loss: 0.1957 Acc: 0.9424 Pre: 0.8920 Rec: 0.9304 F1: 0.9108\n",
      "TP: 2206.0 TN: 4858.0 FP: 267.0 FN: 165.0\n",
      "\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.1228 Acc: 0.9533 Pre: 0.9165 Rec: 0.9235 F1: 0.9200\n",
      "TP: 2547.0 TN: 6489.0 FP: 232.0 FN: 211.0\n",
      "val Loss: 0.0479 Acc: 0.9867 Pre: 0.9828 Rec: 0.9714 F1: 0.9770\n",
      "TP: 2679.0 TN: 6674.0 FP: 47.0 FN: 79.0\n",
      "test Loss: 0.1860 Acc: 0.9461 Pre: 0.9076 Rec: 0.9237 F1: 0.9156\n",
      "TP: 2190.0 TN: 4902.0 FP: 223.0 FN: 181.0\n",
      "\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.1205 Acc: 0.9552 Pre: 0.9195 Rec: 0.9271 F1: 0.9233\n",
      "TP: 2557.0 TN: 6497.0 FP: 224.0 FN: 201.0\n",
      "val Loss: 0.0470 Acc: 0.9865 Pre: 0.9817 Rec: 0.9717 F1: 0.9767\n",
      "TP: 2680.0 TN: 6671.0 FP: 50.0 FN: 78.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1910 Acc: 0.9446 Pre: 0.9025 Rec: 0.9249 F1: 0.9136\n",
      "TP: 2193.0 TN: 4888.0 FP: 237.0 FN: 178.0\n",
      "\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.1201 Acc: 0.9565 Pre: 0.9171 Rec: 0.9351 F1: 0.9260\n",
      "TP: 2579.0 TN: 6488.0 FP: 233.0 FN: 179.0\n",
      "val Loss: 0.0475 Acc: 0.9866 Pre: 0.9803 Rec: 0.9735 F1: 0.9769\n",
      "TP: 2685.0 TN: 6667.0 FP: 54.0 FN: 73.0\n",
      "test Loss: 0.2015 Acc: 0.9414 Pre: 0.8930 Rec: 0.9258 F1: 0.9091\n",
      "TP: 2195.0 TN: 4862.0 FP: 263.0 FN: 176.0\n",
      "\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.1221 Acc: 0.9570 Pre: 0.9221 Rec: 0.9307 F1: 0.9264\n",
      "TP: 2567.0 TN: 6504.0 FP: 217.0 FN: 191.0\n",
      "val Loss: 0.0477 Acc: 0.9866 Pre: 0.9810 Rec: 0.9728 F1: 0.9769\n",
      "TP: 2683.0 TN: 6669.0 FP: 52.0 FN: 75.0\n",
      "test Loss: 0.1929 Acc: 0.9440 Pre: 0.9000 Rec: 0.9258 F1: 0.9127\n",
      "TP: 2195.0 TN: 4881.0 FP: 244.0 FN: 176.0\n",
      "\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.1235 Acc: 0.9564 Pre: 0.9256 Rec: 0.9246 F1: 0.9251\n",
      "TP: 2550.0 TN: 6516.0 FP: 205.0 FN: 208.0\n",
      "val Loss: 0.0541 Acc: 0.9825 Pre: 0.9619 Rec: 0.9786 F1: 0.9702\n",
      "TP: 2699.0 TN: 6614.0 FP: 107.0 FN: 59.0\n",
      "test Loss: 0.2258 Acc: 0.9344 Pre: 0.8609 Rec: 0.9452 F1: 0.9011\n",
      "TP: 2241.0 TN: 4763.0 FP: 362.0 FN: 130.0\n",
      "\n",
      "Training complete in 85m 1s\n",
      "val_train val test\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3436 Acc: 0.8580 Pre: 0.7619 Rec: 0.7447 F1: 0.7532\n",
      "TP: 2054.0 TN: 6079.0 FP: 642.0 FN: 704.0\n",
      "val Loss: 0.1191 Acc: 0.9651 Pre: 0.9687 Rec: 0.9094 F1: 0.9381\n",
      "TP: 2508.0 TN: 6640.0 FP: 81.0 FN: 250.0\n",
      "test Loss: 0.2062 Acc: 0.9365 Pre: 0.9270 Rec: 0.8676 F1: 0.8963\n",
      "TP: 2057.0 TN: 4963.0 FP: 162.0 FN: 314.0\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2541 Acc: 0.9058 Pre: 0.8346 Rec: 0.8434 F1: 0.8390\n",
      "TP: 2326.0 TN: 6260.0 FP: 461.0 FN: 432.0\n",
      "val Loss: 0.1116 Acc: 0.9697 Pre: 0.9574 Rec: 0.9376 F1: 0.9474\n",
      "TP: 2586.0 TN: 6606.0 FP: 115.0 FN: 172.0\n",
      "test Loss: 0.1984 Acc: 0.9364 Pre: 0.9016 Rec: 0.8967 F1: 0.8991\n",
      "TP: 2126.0 TN: 4893.0 FP: 232.0 FN: 245.0\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2234 Acc: 0.9125 Pre: 0.8486 Rec: 0.8513 F1: 0.8500\n",
      "TP: 2348.0 TN: 6302.0 FP: 419.0 FN: 410.0\n",
      "val Loss: 0.0818 Acc: 0.9774 Pre: 0.9743 Rec: 0.9474 F1: 0.9607\n",
      "TP: 2613.0 TN: 6652.0 FP: 69.0 FN: 145.0\n",
      "test Loss: 0.1648 Acc: 0.9490 Pre: 0.9352 Rec: 0.9013 F1: 0.9180\n",
      "TP: 2137.0 TN: 4977.0 FP: 148.0 FN: 234.0\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2079 Acc: 0.9202 Pre: 0.8596 Rec: 0.8677 F1: 0.8636\n",
      "TP: 2393.0 TN: 6330.0 FP: 391.0 FN: 365.0\n",
      "val Loss: 0.0886 Acc: 0.9729 Pre: 0.9500 Rec: 0.9572 F1: 0.9536\n",
      "TP: 2640.0 TN: 6582.0 FP: 139.0 FN: 118.0\n",
      "test Loss: 0.2517 Acc: 0.9138 Pre: 0.8241 Rec: 0.9249 F1: 0.8716\n",
      "TP: 2193.0 TN: 4657.0 FP: 468.0 FN: 178.0\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.1985 Acc: 0.9236 Pre: 0.8650 Rec: 0.8738 F1: 0.8694\n",
      "TP: 2410.0 TN: 6345.0 FP: 376.0 FN: 348.0\n",
      "val Loss: 0.0925 Acc: 0.9740 Pre: 0.9505 Rec: 0.9608 F1: 0.9556\n",
      "TP: 2650.0 TN: 6583.0 FP: 138.0 FN: 108.0\n",
      "test Loss: 0.2556 Acc: 0.9282 Pre: 0.8505 Rec: 0.9380 F1: 0.8921\n",
      "TP: 2224.0 TN: 4734.0 FP: 391.0 FN: 147.0\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.1798 Acc: 0.9304 Pre: 0.8771 Rec: 0.8847 F1: 0.8809\n",
      "TP: 2440.0 TN: 6379.0 FP: 342.0 FN: 318.0\n",
      "val Loss: 0.0680 Acc: 0.9792 Pre: 0.9638 Rec: 0.9648 F1: 0.9643\n",
      "TP: 2661.0 TN: 6621.0 FP: 100.0 FN: 97.0\n",
      "test Loss: 0.2126 Acc: 0.9345 Pre: 0.8683 Rec: 0.9346 F1: 0.9003\n",
      "TP: 2216.0 TN: 4789.0 FP: 336.0 FN: 155.0\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.1487 Acc: 0.9439 Pre: 0.8955 Rec: 0.9137 F1: 0.9045\n",
      "TP: 2520.0 TN: 6427.0 FP: 294.0 FN: 238.0\n",
      "val Loss: 0.0612 Acc: 0.9823 Pre: 0.9758 Rec: 0.9630 F1: 0.9693\n",
      "TP: 2656.0 TN: 6655.0 FP: 66.0 FN: 102.0\n",
      "test Loss: 0.2218 Acc: 0.9348 Pre: 0.8719 Rec: 0.9304 F1: 0.9002\n",
      "TP: 2206.0 TN: 4801.0 FP: 324.0 FN: 165.0\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1444 Acc: 0.9462 Pre: 0.9084 Rec: 0.9065 F1: 0.9074\n",
      "TP: 2500.0 TN: 6469.0 FP: 252.0 FN: 258.0\n",
      "val Loss: 0.0546 Acc: 0.9830 Pre: 0.9713 Rec: 0.9703 F1: 0.9708\n",
      "TP: 2676.0 TN: 6642.0 FP: 79.0 FN: 82.0\n",
      "test Loss: 0.2149 Acc: 0.9358 Pre: 0.8703 Rec: 0.9367 F1: 0.9023\n",
      "TP: 2221.0 TN: 4794.0 FP: 331.0 FN: 150.0\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1415 Acc: 0.9499 Pre: 0.9093 Rec: 0.9195 F1: 0.9144\n",
      "TP: 2536.0 TN: 6468.0 FP: 253.0 FN: 222.0\n",
      "val Loss: 0.0524 Acc: 0.9840 Pre: 0.9728 Rec: 0.9721 F1: 0.9724\n",
      "TP: 2681.0 TN: 6646.0 FP: 75.0 FN: 77.0\n",
      "test Loss: 0.2315 Acc: 0.9309 Pre: 0.8581 Rec: 0.9363 F1: 0.8955\n",
      "TP: 2220.0 TN: 4758.0 FP: 367.0 FN: 151.0\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1318 Acc: 0.9518 Pre: 0.9161 Rec: 0.9184 F1: 0.9173\n",
      "TP: 2533.0 TN: 6489.0 FP: 232.0 FN: 225.0\n",
      "val Loss: 0.0512 Acc: 0.9853 Pre: 0.9830 Rec: 0.9663 F1: 0.9746\n",
      "TP: 2665.0 TN: 6675.0 FP: 46.0 FN: 93.0\n",
      "test Loss: 0.2011 Acc: 0.9404 Pre: 0.8857 Rec: 0.9317 F1: 0.9081\n",
      "TP: 2209.0 TN: 4840.0 FP: 285.0 FN: 162.0\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "path = 'models/transfer_learning'\n",
    "for seed in range(5):\n",
    "    torch.manual_seed(seed)\n",
    "    model_ts = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ts.fc.in_features\n",
    "    model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "    model_ts = model_ts.to(device)\n",
    "\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.]).to(device))\n",
    "#     criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([5.]).to(device))\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ts = optim.SGD(model_ts.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler_ts = lr_scheduler.StepLR(optimizer_ts, step_size=7, gamma=0.1)\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(os.path.join(path, 'seed_{}.log'.format(seed)), 'w') as log_file:\n",
    "        model_ts = train_model(model_ts, criterion, optimizer_ts, exp_lr_scheduler_ts,\n",
    "                               'val_train', 'val', test_dl='test', num_epochs=25, verbose=True,\n",
    "                               log_file=log_file, return_best=False)\n",
    "        torch.save(model_ts.state_dict(), os.path.join(path, 'seed_{}.pth'.format(seed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smooth Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_negs(predictions, gt):\n",
    "    correct = np.where(np.array(predictions) == np.array(gt), 1, 0)\n",
    "    incorrect = np.where(np.array(predictions) == np.array(gt), 0, 1)\n",
    "    \n",
    "    tp = np.where(correct * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    tn = np.where(correct * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    fp = np.where(incorrect * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    fn = np.where(incorrect * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def acc_prf1(predictions, gt):\n",
    "    tp, tn, fp, fn = pos_negs(predictions, gt)\n",
    "    \n",
    "    acc = (np.sum(tp) + np.sum(tn)) / (np.sum(tp) + np.sum(tn) + np.sum(fp) + np.sum(fn))\n",
    "    precision = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return acc, precision, recall, f1, np.sum(tp), np.sum(tn), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def smooth_predictions(preds, window_radius = 3):\n",
    "    result = []\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_radius)\n",
    "        end = min(len(preds), i + window_radius)\n",
    "        window = preds[start:end]\n",
    "        result += [max(window, key=window.count)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Acc: 0.9400 Pre: 0.8912 Rec: 0.9228 F1: 0.9068\n",
      "TP: 2188.0 TN: 4858.0 FP: 267.0 FN: 183.0\n",
      "Smoothed stats:\n",
      "(0.9340981856990395, 0.8813490451036164, 0.9148038802193167, 0.8977649006622517, 2169, 4833, 292, 202)\n",
      "1\n",
      "Acc: 0.9506 Pre: 0.9216 Rec: 0.9224 F1: 0.9220\n",
      "TP: 2187.0 TN: 4939.0 FP: 186.0 FN: 184.0\n",
      "Smoothed stats:\n",
      "(0.9450373532550693, 0.9145154464663563, 0.9114297764656263, 0.912970004224757, 2161, 4923, 202, 210)\n",
      "2\n",
      "Acc: 0.9452 Pre: 0.8911 Rec: 0.9418 F1: 0.9157\n",
      "TP: 2233.0 TN: 4852.0 FP: 273.0 FN: 138.0\n",
      "Smoothed stats:\n",
      "(0.9389007470651014, 0.8883475436459602, 0.9228173766343315, 0.9052544476623914, 2188, 4850, 275, 183)\n",
      "3\n",
      "Acc: 0.9344 Pre: 0.8609 Rec: 0.9452 F1: 0.9011\n",
      "TP: 2241.0 TN: 4763.0 FP: 362.0 FN: 130.0\n",
      "Smoothed stats:\n",
      "(0.9288954108858057, 0.8601097178683386, 0.9257697174188106, 0.8917326833231769, 2195, 4768, 357, 176)\n",
      "4\n",
      "Acc: 0.9338 Pre: 0.8692 Rec: 0.9308 F1: 0.8990\n",
      "TP: 2207.0 TN: 4793.0 FP: 332.0 FN: 164.0\n",
      "Smoothed stats:\n",
      "(0.9303628601921025, 0.8678869876641464, 0.9198650358498524, 0.893120393120393, 2181, 4793, 332, 190)\n"
     ]
    }
   ],
   "source": [
    "path = 'models/transfer_learning'\n",
    "\n",
    "log_file_img = open(os.path.join(\n",
    "    path, 'test_results_image_classifier.log'), 'w')\n",
    "log_file_smoothed = open(os.path.join(\n",
    "    path, 'test_results_smoothed.log'), 'w')\n",
    "\n",
    "for seed in range(5):\n",
    "    print(seed)\n",
    "    model_ts = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ts.fc.in_features\n",
    "    model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "    model_ts.load_state_dict(torch.load(\n",
    "        os.path.join(path, 'seed_{}.pth'.format(seed))\n",
    "    ))\n",
    "    model_ts.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model = model_ts.eval()   # Set model to evaluate mode\n",
    "    dl = dataloaders['test']\n",
    "    dataset_size = dataset_sizes['test']\n",
    "    \n",
    "    running_corrects = 0\n",
    "    true_positives = 0.\n",
    "    true_negatives = 0.\n",
    "    false_positives = 0.\n",
    "    false_negatives = 0.\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    predictions = []\n",
    "    gt_labels = []\n",
    "    # Iterate over data.\n",
    "    for inputs, labels in dl:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.where(\n",
    "                outputs >= 0.,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "            target = torch.where(\n",
    "                labels >= 0.5,\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device)\n",
    "            )\n",
    "\n",
    "        predictions += preds.cpu().numpy().tolist()\n",
    "        gt_labels += labels.cpu().numpy().tolist()\n",
    "\n",
    "        # statistics\n",
    "        label_vals = torch.where(\n",
    "            labels >= 0.5,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device)\n",
    "        )\n",
    "        correct = preds.view(label_vals.shape) == label_vals.data\n",
    "        running_corrects += torch.sum(correct)\n",
    "\n",
    "        true_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        true_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 1.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_positives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 0.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "        false_negatives += torch.sum(\n",
    "            torch.where(\n",
    "                (correct == 0.) * (label_vals == 1.),\n",
    "                torch.tensor([1.]).to(device),\n",
    "                torch.tensor([0.]).to(device))\n",
    "        )\n",
    "\n",
    "        num_fp = torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "        )\n",
    "\n",
    "    #     if num_fp > 0:\n",
    "    #         print(num_fp)\n",
    "    #         print(torch.where(\n",
    "    #             (correct == 0.) * (label_vals == 0.),\n",
    "    #             torch.tensor([1.]).to(device),\n",
    "    #             torch.tensor([0.]).to(device)))\n",
    "    #         print(i)\n",
    "    #         out = torchvision.utils.make_grid(inputs)\n",
    "    # #         imshow(out, title=preds.cpu().numpy().tolist())\n",
    "    #         imshow(out)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    epoch_acc = running_corrects.double() / dataset_size\n",
    "    epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "    epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "    epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "    print('Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "        epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "    print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "        true_positives.data, true_negatives.data, false_positives.data, \n",
    "        false_negatives.data))\n",
    "    \n",
    "    log_file_img.write('Seed: {0}\\t'\n",
    "                   'Acc: {acc:.4f}\\t'\n",
    "                   'Pre: {pre:.4f}\\t'\n",
    "                   'Rec: {rec:.4f}\\t'\n",
    "                   'F1: {f1:.4f}\\t'\n",
    "                   'TP: {tp} '\n",
    "                   'TN: {tn} '\n",
    "                   'FP: {fp} '\n",
    "                   'FN: {fn}\\n'.format(\n",
    "                       seed,\n",
    "                       acc=epoch_acc, pre=epoch_pre, rec=epoch_recall,\n",
    "                       f1=epoch_f1, tp=int(true_positives.data),\n",
    "                       tn=int(true_negatives.data),\n",
    "                       fp=int(false_positives.data), fn=int(false_negatives.data)\n",
    "                   ))\n",
    "    log_file_img.flush()\n",
    "\n",
    "    predictions = [p[0] for p in predictions]\n",
    "\n",
    "    smoothed_preds = smooth_predictions(predictions, 3)\n",
    "\n",
    "    print(\"Smoothed stats:\")\n",
    "    print(acc_prf1(smoothed_preds, gt_labels))\n",
    "    \n",
    "    sm_acc, sm_pre, sm_rec, sm_f1, sm_tp, sm_tn, sm_fp, sm_fn = acc_prf1(\n",
    "        smoothed_preds, gt_labels)\n",
    "    \n",
    "    log_file_smoothed.write('Seed: {0}\\t'\n",
    "                   'Acc: {acc:.4f}\\t'\n",
    "                   'Pre: {pre:.4f}\\t'\n",
    "                   'Rec: {rec:.4f}\\t'\n",
    "                   'F1: {f1:.4f}\\t'\n",
    "                   'TP: {tp} '\n",
    "                   'TN: {tn} '\n",
    "                   'FP: {fp} '\n",
    "                   'FN: {fn}\\n'.format(\n",
    "                       seed,\n",
    "                       acc=sm_acc, pre=sm_pre, rec=sm_rec,\n",
    "                       f1=sm_f1, tp=sm_tp,\n",
    "                       tn=sm_tn,\n",
    "                       fp=sm_fp, fn=sm_fn\n",
    "                   ))\n",
    "    log_file_smoothed.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/transfer_learning'\n",
    "model_ts = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ts.fc.in_features\n",
    "model_ts.fc = nn.Linear(num_ftrs, 1)\n",
    "model_ts.load_state_dict(torch.load(\n",
    "    os.path.join(path, 'seed_0.pth')\n",
    "))\n",
    "model_ts.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([3.]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_ts.eval()   # Set model to evaluate mode\n",
    "dl = dataloaders['test']\n",
    "dataset_size = dataset_sizes['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.cpu().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(str(title))\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_negs(predictions, gt):\n",
    "    correct = np.where(np.array(predictions) == np.array(gt), 1, 0)\n",
    "    incorrect = np.where(np.array(predictions) == np.array(gt), 0, 1)\n",
    "    \n",
    "    tp = np.where(correct * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    tn = np.where(correct * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    fp = np.where(incorrect * np.where(predictions == np.array(1), 1, 0), 1, 0)\n",
    "    fn = np.where(incorrect * np.where(predictions == np.array(0), 1, 0), 1, 0)\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def acc_prf1(predictions, gt):\n",
    "    tp, tn, fp, fn = pos_negs(predictions, gt)\n",
    "    \n",
    "    acc = (np.sum(tp) + np.sum(tn)) / (np.sum(tp) + np.sum(tn) + np.sum(fp) + np.sum(fn))\n",
    "    precision = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "    recall = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return acc, precision, recall, f1, np.sum(tp), np.sum(tn), np.sum(fp), np.sum(fn)\n",
    "\n",
    "def smooth_predictions(preds, window_radius = 3):\n",
    "    result = []\n",
    "    for i in range(len(preds)):\n",
    "        start = max(0, i - window_radius)\n",
    "        end = min(len(preds), i + window_radius)\n",
    "        window = preds[start:end]\n",
    "        result += [max(window, key=window.count)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9421 Pre: 0.8904 Rec: 0.9317 F1: 0.9106\n",
      "TP: 2209.0 TN: 4853.0 FP: 272.0 FN: 162.0\n",
      "Smoothed stats:\n",
      "(0.9327641408751334, 0.8783948115119579, 0.9139603542808942, 0.8958247209590741, 2167, 4825, 300, 204)\n"
     ]
    }
   ],
   "source": [
    "running_corrects = 0\n",
    "true_positives = 0.\n",
    "true_negatives = 0.\n",
    "false_positives = 0.\n",
    "false_negatives = 0.\n",
    "\n",
    "i = 0\n",
    "\n",
    "predictions = []\n",
    "gt_labels = []\n",
    "# Iterate over data.\n",
    "for inputs, labels in dl:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device).float()\n",
    "\n",
    "    # forward\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.where(\n",
    "            outputs >= 0.,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "        target = torch.where(\n",
    "            labels >= 0.5,\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device)\n",
    "        )\n",
    "        \n",
    "    predictions += preds.cpu().numpy().tolist()\n",
    "    gt_labels += labels.cpu().numpy().tolist()\n",
    "\n",
    "    # statistics\n",
    "    label_vals = torch.where(\n",
    "        labels >= 0.5,\n",
    "        torch.tensor([1.]).to(device),\n",
    "        torch.tensor([0.]).to(device)\n",
    "    )\n",
    "    correct = preds.view(label_vals.shape) == label_vals.data\n",
    "    running_corrects += torch.sum(correct)\n",
    "\n",
    "    true_positives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 1.) * (label_vals == 1.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    true_negatives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 1.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    false_positives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 0.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    false_negatives += torch.sum(\n",
    "        torch.where(\n",
    "            (correct == 0.) * (label_vals == 1.),\n",
    "            torch.tensor([1.]).to(device),\n",
    "            torch.tensor([0.]).to(device))\n",
    "    )\n",
    "    \n",
    "    num_fp = torch.sum(\n",
    "    torch.where(\n",
    "        (correct == 0.) * (label_vals == 0.),\n",
    "        torch.tensor([1.]).to(device),\n",
    "        torch.tensor([0.]).to(device))\n",
    "    )\n",
    "\n",
    "#     if num_fp > 0:\n",
    "#         print(num_fp)\n",
    "#         print(torch.where(\n",
    "#             (correct == 0.) * (label_vals == 0.),\n",
    "#             torch.tensor([1.]).to(device),\n",
    "#             torch.tensor([0.]).to(device)))\n",
    "#         print(i)\n",
    "#         out = torchvision.utils.make_grid(inputs)\n",
    "# #         imshow(out, title=preds.cpu().numpy().tolist())\n",
    "#         imshow(out)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "epoch_acc = running_corrects.double() / dataset_size\n",
    "epoch_pre = safe_divide(true_positives, (true_positives + false_positives))\n",
    "epoch_recall = safe_divide(true_positives, (true_positives + false_negatives))\n",
    "epoch_f1 = safe_divide(2 * epoch_pre * epoch_recall, (epoch_pre + epoch_recall))\n",
    "\n",
    "print('Acc: {:.4f} Pre: {:.4f} Rec: {:.4f} F1: {:.4f}'.format(\n",
    "    epoch_acc, epoch_pre, epoch_recall, epoch_f1))\n",
    "print('TP: {} TN: {} FP: {} FN: {}'.format(\n",
    "    true_positives.data, true_negatives.data, false_positives.data, false_negatives.data))\n",
    "\n",
    "predictions = [p[0] for p in predictions]\n",
    "\n",
    "smoothed_preds = smooth_predictions(predictions, 3)\n",
    "\n",
    "print(\"Smoothed stats:\")\n",
    "print(acc_prf1(smoothed_preds, gt_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sequential_ws] *",
   "language": "python",
   "name": "conda-env-sequential_ws-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
